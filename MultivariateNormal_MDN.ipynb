{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from scipy.stats import beta\n",
    "import scipy.special\n",
    "from utils import *\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a MDN for approximating a multidimensional Gaussian\n",
    "\n",
    "It takes as input the data $x$ **and** the model index $m$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=10, n_components=1):\n",
    "        super(MDN, self).__init__()\n",
    "        \n",
    "        self.ndims = ndim_output \n",
    "        # the number of entries in the upper triangular Choleski transform matrix of the precision matrix \n",
    "        self.utriu_entries = int(self.ndims * (self.ndims - 1) / 2) + self.ndims\n",
    "\n",
    "        # input layer \n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        # activation \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # output layer the mean estimates \n",
    "        self.mu_out = nn.Linear(n_hidden, ndim_output)\n",
    "\n",
    "        # output layer to precision estimates \n",
    "        # the upper triangular matrix for D-dim Gaussian has m = (D**2 + D) / 2 entries \n",
    "        # this should be a m-vector for every component. currently it is just a scalar for every component. \n",
    "        # or it could be a long vector of length m * k, i.e, all the k vector stacked. \n",
    "        self.U_out = nn.Linear(n_hidden, self.ndims**2 * n_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        \n",
    "        out_mu = self.mu_out(act)\n",
    "\n",
    "        # get activate of upper triangle U vector\n",
    "        U_vec = self.U_out(act)\n",
    "        # prelocate U matrix \n",
    "        \"\"\"\n",
    "        U_mat = Variable(torch.zeros(batch_size, self.ndims, self.ndims))\n",
    "        \n",
    "        # assign vector to upper triangle of U \n",
    "        (idx1, idx2) = np.triu_indices(self.ndims)\n",
    "        U_mat[:, idx1, idx2] = U_vec\n",
    "        # apply exponential to get positive diagonal\n",
    "        (idx1, idx2) = np.diag_indices(self.ndims)\n",
    "        U_mat[:, idx1, idx2] = torch.exp(U_mat[:, idx1, idx2])\n",
    "        \"\"\"\n",
    "        lower_triangular_mask = Variable(torch.ones(self.ndims, self.ndims))\n",
    "        diagonal_mask = Variable(torch.eye(self.ndims))\n",
    "        U_mat = lower_triangular_mask * U_vec.view(batch_size, self.ndims, self.ndims) + diagonal_mask * torch.exp(U_vec.view(batch_size, self.ndims, self.ndims))\n",
    "        \n",
    "        return (out_mu, U_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this one is used to approximate the posterior with a mixture of Gaussians \n",
    "def ND_gauss_pdf(X, mus, Us, log=False):\n",
    "    # dimension of the Gaussian \n",
    "    D = mus.size()[1]\n",
    "    N = mus.size()[0]\n",
    "    \n",
    "    # get the precision matrices over batches using matrix multiplication: S^-1 = U'U\n",
    "    Sin = torch.bmm(torch.transpose(Us, 1, 2), Us)\n",
    "    \n",
    "    norm_const = Variable(torch.zeros(N, 1))\n",
    "    log_probs = Variable(torch.zeros(N, 1))\n",
    "    \n",
    "    for idx in range(N): \n",
    "        diagU = torch.diag(Us[idx, ])\n",
    "        norm_const[idx] = (torch.sum(torch.log(diagU), -1) - (D / 2) * np.log(2 * np.pi)).unsqueeze(-1)\n",
    "\n",
    "        diff = (X[idx, ] - mus[idx, ]).unsqueeze(-1)\n",
    "        log_probs[idx] = - 0.5 * torch.mm(torch.transpose(diff, 0, 1), torch.mm(Sin[idx, ], diff))\n",
    "        \n",
    "    ps = norm_const + log_probs\n",
    "    log_probs = ps \n",
    "    \n",
    "    if log:\n",
    "        return log_probs\n",
    "    else: \n",
    "        return torch.exp(log_probs)\n",
    "\n",
    "# the loss evaluates model (MoG) with the given data (y) and takes the log loss\n",
    "def mdn_loss_function(y, mu, U):\n",
    "    \n",
    "#    result = ND_gauss_pdf(y, mu, U, log=True)\n",
    "    result = multivariate_normal_pdf(y, mu, U, log=True)    \n",
    "    \n",
    "    result = torch.mean(result)  # mean over batch\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_2DGaussian_dataset(n_samples, sample_size, prior):\n",
    "\n",
    "    X = []\n",
    "    thetas = []\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        # sample from the prior \n",
    "        theta = prior.rvs()\n",
    "\n",
    "        # generate samples with mean from prior and unit variance \n",
    "        x = scipy.stats.multivariate_normal.rvs(mean=theta, cov=.5 * np.eye(2), size=sample_size).reshape(sample_size, 2)\n",
    "        \n",
    "        sx = stats_ND_Gaussian(x)\n",
    "\n",
    "        # as data we append the summary stats\n",
    "        X.append(sx) \n",
    "        thetas.append([theta])   \n",
    "    \n",
    "    return np.array(X).squeeze(), np.array(thetas).squeeze()\n",
    "\n",
    "def stats_ND_Gaussian(x): \n",
    "    \"\"\"\n",
    "    Calculate the sufficient statistics of a multivariate Gaussian sample x\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(x, axis=0).astype(float)])\n",
    "\n",
    "def batch_generator(dataset, batch_size=5):\n",
    "    shuffle(dataset)\n",
    "    N_full_batches = len(dataset) // batch_size\n",
    "    for i in range(N_full_batches):\n",
    "        idx_from = batch_size * i\n",
    "        idx_to = batch_size * (i + 1)\n",
    "        xs, ys = zip(*[(x, y) for x, y in dataset[idx_from:idx_to]])\n",
    "        yield xs, ys\n",
    "        \n",
    "def train(X, Y, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))\n",
    "            y_var = Variable(torch.Tensor(y_batch))\n",
    "                        \n",
    "            (out_mu, out_U) = model(x_var)\n",
    "            loss = mdn_loss_function(y_var, out_mu, out_U)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data.numpy())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "            \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MDN(ndim_input=2, n_components=1)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "sample_size = 1000\n",
    "    \n",
    "# prior on the mean\n",
    "prior = scipy.stats.multivariate_normal(mean=[0., 0.], cov=2. * np.eye(2))\n",
    "\n",
    "X, Y = generate_2DGaussian_dataset(n_samples, sample_size, prior)\n",
    "X, norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train(X, Y, n_epochs=50, n_minibatch=100)\n",
    "plt.plot(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# observe data \n",
    "true_mu = [-.5, .1]\n",
    "true_cov = .5 * np.eye(2)\n",
    "xo = scipy.stats.multivariate_normal.rvs(mean=true_mu, cov=true_cov, size=sample_size).reshape(sample_size, 2)\n",
    "# gets stats \n",
    "statso = stats_ND_Gaussian(xo)\n",
    "# normalize \n",
    "statso, norm = normalize(statso, norm)\n",
    "# cast to torch \n",
    "statso = Variable(torch.Tensor(statso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(211)\n",
    "plt.hist2d(X[:, 0], X[:, 1])\n",
    "plt.title('Samples generated by the model: 2D Gaussian with prior on $\\mu$');\n",
    "plt.colorbar();\n",
    "plt.subplot(212)\n",
    "plt.title('observed data')\n",
    "plt.hist2d(xo[:, 0], xo[:, 1])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with observed data \n",
    "(out_mu, out_U) = model(statso.view(1, 2))\n",
    "Sin = torch.mm(torch.transpose(out_U.view(2, 2), 0, 1), out_U.view(2, 2))\n",
    "\n",
    "# convert to numpy \n",
    "mean = out_mu.data.numpy().squeeze()\n",
    "cov = torch.inverse(Sin).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posterior = scipy.stats.multivariate_normal(mean=mean, cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the true posterior using the analytical result: \n",
    "prior_precision = np.linalg.inv(prior.cov)\n",
    "prior_mean = prior.mean\n",
    "\n",
    "# calculate mean and cov of data \n",
    "data_cov = true_cov\n",
    "data_precision = np.linalg.inv(data_cov)\n",
    "data_mean = xo[0, :].reshape(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_cov = np.linalg.inv(prior_precision + data_precision)\n",
    "post_mean = post_cov.dot(data_cov.dot(data_mean.T) + prior_precision.dot(prior_mean).reshape(2, -1)).squeeze()\n",
    "\n",
    "postana = scipy.stats.multivariate_normal(mean=post_mean, cov=post_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative updating of the posterior\n",
    "for idx in range(1, sample_size): \n",
    "    data = xo[idx, :].reshape(1, 2)\n",
    "    prior_precision = np.linalg.inv(postana.cov)\n",
    "    prior_mean = postana.mean\n",
    "    \n",
    "    post_cov = np.linalg.inv(prior_precision + data_precision)\n",
    "    post_mean = post_cov.dot(data_cov.dot(data.T) + prior_precision.dot(prior_mean).reshape(2, -1)).squeeze()\n",
    "\n",
    "    postana = scipy.stats.multivariate_normal(mean=post_mean, cov=post_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1.\n",
    "x, y = np.mgrid[-r:r:.01, -r:r:.01]\n",
    "pos = np.dstack((x, y))\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.subplot(311)\n",
    "plt.contourf(x, y, prior.pdf(pos))\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro')\n",
    "plt.title('Prior')\n",
    "plt.subplot(312)\n",
    "plt.contourf(x, y, posterior.pdf(pos))\n",
    "plt.title('Posterior')\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro');\n",
    "plt.subplot(313)\n",
    "plt.contourf(x, y, postana.pdf(pos))\n",
    "plt.title('Analytical Posterior')\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
