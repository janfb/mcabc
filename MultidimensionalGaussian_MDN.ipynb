{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from scipy.stats import beta\n",
    "import scipy.special\n",
    "from utils import *\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a MDN for approximating a multidimensional Gaussian\n",
    "\n",
    "It takes as input the data $x$ **and** the model index $m$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=20, n_components=1):\n",
    "        super(MDN, self).__init__()\n",
    "        \n",
    "        self.ndims = ndim_output \n",
    "        # the number of entries in the upper triangular Choleski transform matrix of the precision matrix \n",
    "        self.utriu_entries = int(self.ndims * (self.ndims - 1) / 2) + self.ndims\n",
    "\n",
    "        # input layer \n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        # activation \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # output layer the mean estimates \n",
    "        self.mu_out = nn.Linear(n_hidden, ndim_output)\n",
    "\n",
    "        # output layer to precision estimates \n",
    "        # the upper triangular matrix for D-dim Gaussian has m = (D**2 + D) / 2 entries \n",
    "        # this should be a m-vector for every component. currently it is just a scalar for every component. \n",
    "        # or it could be a long vector of length m * k, i.e, all the k vector stacked. \n",
    "        self.U_out = nn.Linear(n_hidden, self.utriu_entries * n_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        \n",
    "        out_mu = self.mu_out(act)\n",
    "\n",
    "        # get activate of upper triangle U vector\n",
    "        U_vec = self.U_out(act)\n",
    "        # prelocate U matrix \n",
    "        U_mat = Variable(torch.zeros(batch_size, self.ndims, self.ndims))\n",
    "        \n",
    "        # assign vector to upper triangle of U \n",
    "        (idx1, idx2) = np.triu_indices(self.ndims)\n",
    "        U_mat[:, idx1, idx2] = U_vec\n",
    "        # apply exponential to get positive diagonal\n",
    "        (idx1, idx2) = np.diag_indices(self.ndims)\n",
    "        U_mat[:, idx1, idx2] = torch.exp(U_mat[:, idx1, idx2])\n",
    "\n",
    "        return (out_mu, U_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this one is used to approximate the posterior with a mixture of Gaussians \n",
    "def ND_gauss_pdf(X, mus, Us, log=False):\n",
    "    # dimension of the Gaussian \n",
    "    D = mus.size()[1]\n",
    "    N = mus.size()[0]\n",
    "    \n",
    "    # get the precision matrices over batches using matrix multiplication: S^-1 = U'U\n",
    "    Sin = torch.bmm(torch.transpose(Us, 1, 2), Us)\n",
    "    \n",
    "    norm_const = Variable(torch.zeros(N))\n",
    "    \n",
    "    for idx in range(N): \n",
    "        norm_const[idx] = (torch.log(torch.diag(Us[idx, ])).sum(-1) + (D / 2) * np.log(2 * np.pi)).unsqueeze(-1)\n",
    "            \n",
    "    lp = 0.5 * torch.sum((X - mus).unsqueeze(-1) * torch.bmm(Sin, (X - mus).unsqueeze(-1)), dim=1)\n",
    "    ps = -(norm_const + lp)\n",
    "    log_probs = ps \n",
    "    \n",
    "    if log:\n",
    "        return log_probs\n",
    "    else: \n",
    "        return torch.exp(log_probs)\n",
    "\n",
    "# the loss evaluates model (MoG) with the given data (y) and takes the log loss\n",
    "def mdn_loss_function(y, mu, U):\n",
    "    \n",
    "    result = ND_gauss_pdf(y, mu, U, log=True)\n",
    "    \n",
    "    result = torch.mean(result)  # mean over batch\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_U(diagU, utriU): \n",
    "    n_dims = diagU.size()[0]\n",
    "    \n",
    "    # prelocate U \n",
    "    U = torch.zeros(n_dims, n_dims)\n",
    "    \n",
    "    # get index tuple for upper triangle\n",
    "    if n_dims > 2: \n",
    "        i1, i2 = np.triu_indices(n_dims - 1)\n",
    "        i2 += 1\n",
    "        U[(i1, i2)] = utriU.data\n",
    "    elif n_dims == 2:\n",
    "        U[0, 1] = float(utriU.data.numpy()[0])\n",
    "\n",
    "    # set diag values \n",
    "    U[np.diag_indices(n_dims)] = diagU.data\n",
    "    \n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_2DGaussian_dataset(n_samples, sample_size):\n",
    "\n",
    "    X = []\n",
    "    thetas = []\n",
    "    \n",
    "    # prior on the mean\n",
    "    prior = scipy.stats.multivariate_normal(mean=[.2, .2], cov=[[.1, 0.], [0., 0.1]])\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        # sample from the prior \n",
    "        theta = prior.rvs()\n",
    "\n",
    "        # generate samples with mean from prior and unit variance \n",
    "        x = scipy.stats.multivariate_normal.rvs(mean=theta, cov=np.eye(2), size=n_samples)\n",
    "        \n",
    "        # as data we append the summary stats\n",
    "        X.append(stats_ND_Gaussian(x)) \n",
    "        thetas.append([theta])\n",
    "    \n",
    "    return np.array(X).squeeze(), np.array(thetas).squeeze()\n",
    "\n",
    "def stats_ND_Gaussian(x): \n",
    "    \"\"\"\n",
    "    Calculate the sufficient statistics of a multivariate Gaussian sample x\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(x, axis=0).astype(float)])\n",
    "\n",
    "def batch_generator(dataset, batch_size=5):\n",
    "    shuffle(dataset)\n",
    "    N_full_batches = len(dataset) // batch_size\n",
    "    for i in range(N_full_batches):\n",
    "        idx_from = batch_size * i\n",
    "        idx_to = batch_size * (i + 1)\n",
    "        xs, ys = zip(*[(x, y) for x, y in dataset[idx_from:idx_to]])\n",
    "        yield xs, ys\n",
    "        \n",
    "def train(X, Y, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))\n",
    "            y_var = Variable(torch.Tensor(y_batch))\n",
    "                        \n",
    "            (out_mu, out_U) = model(x_var)\n",
    "            loss = mdn_loss_function(y_var, out_mu, out_U)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data.numpy())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "            \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MDN(ndim_input=2, n_components=1)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = generate_2DGaussian_dataset(100, 10)\n",
    "X, norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train(X, Y, n_epochs=50, n_minibatch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# observe data \n",
    "true_mu = [0.5, 0.5]\n",
    "xo = scipy.stats.multivariate_normal.rvs(mean=true_mu, cov=np.eye(2), size=10)\n",
    "# gets stats \n",
    "statso = stats_ND_Gaussian(xo)\n",
    "# normalize \n",
    "statso, norm = normalize(statso, norm)\n",
    "# cast to torch \n",
    "statso = Variable(torch.Tensor(statso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(out_mu, out_utriU, out_diagU) = model(statso)\n",
    "U = get_U(out_diagU.squeeze(), out_utriU)\n",
    "Sin = torch.mm(torch.transpose(U, 0, 1), U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
