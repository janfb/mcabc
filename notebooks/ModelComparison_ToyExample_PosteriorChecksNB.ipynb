{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "import sys \n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import *\n",
    "from model_comparison.models import PoissonModel, NegativeBinomialModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpl_params = {'legend.fontsize': 14,\n",
    "              'legend.frameon': False,\n",
    "                      'axes.titlesize': 20,\n",
    "                      'axes.labelsize': 17,\n",
    "                      'xtick.labelsize': 12,\n",
    "                      'ytick.labelsize': 12,\n",
    "             'figure.figsize' : (18, 5)}\n",
    "\n",
    "mpl.rcParams.update(mpl_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained NB posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = '../data/'\n",
    "fn = 'learned_posterior_nbmodel_ntrain100000.p'\n",
    "time_stamp = time.strftime('%Y%m%d%H%M_')\n",
    "\n",
    "with open(os.path.join(folder, fn), 'rb') as f: \n",
    "    d = pickle.load(f)\n",
    "    \n",
    "# set the seed for generating new test data \n",
    "seed = 3\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# priors \n",
    "prior_k = d['prior_k']\n",
    "prior_theta = d['prior_theta']\n",
    "sample_size = d['sample_size']\n",
    "ntrain = d['ntrain']\n",
    "param_norm = d['param_norm']\n",
    "data_norm = d['data_norm']\n",
    "\n",
    "model_nb = d['model']\n",
    "model_params_mdn = d['mdn']\n",
    "loss_trace = d['trainer'].loss_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(loss_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample new test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntest = 100\n",
    "params_test = np.vstack((prior_k.rvs(size=ntest), prior_theta.rvs(size=ntest))).T\n",
    "x_test = model_nb.gen(params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sx_test = calculate_stats_toy_examples(x_test)\n",
    "sx_test_zt, _ = normalize(sx_test, data_norm)\n",
    "params_test_zt, _ = normalize(params_test, param_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over test samples for \n",
    "- #### Quantiles \n",
    "- #### posterior mean differences \n",
    "- #### $D_{KL}$\n",
    "- #### credible intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quantiles, dkl ratios and credible intervals for every dimension (2)\n",
    "dkl_ratios = np.zeros((3, ntest))\n",
    "\n",
    "qis = np.zeros_like(dkl_ratios)\n",
    "qis_hat = np.zeros_like(dkl_ratios)\n",
    "\n",
    "mus_hat = np.zeros_like(dkl_ratios)\n",
    "mus_exact = np.zeros_like(dkl_ratios)\n",
    "stds_hat = np.zeros_like(dkl_ratios)\n",
    "stds_exact = np.zeros_like(dkl_ratios)\n",
    "\n",
    "credible_intervals = np.arange(0.05, 1., 0.05)\n",
    "marginal_ci_counts = np.zeros((2, ntest, credible_intervals.size))\n",
    "marginal_ci_counts_hat = np.zeros((2, ntest, credible_intervals.size))\n",
    "\n",
    "covariances = []\n",
    "covariances_hat = []\n",
    "\n",
    "ms = []\n",
    "ms_hat = []\n",
    "ps = []\n",
    "ps_hat = []\n",
    "\n",
    "priors = [prior_k, prior_theta]\n",
    "joint_prior = JointGammaPrior(prior_k, prior_theta)\n",
    "# for every test sample \n",
    "fails = []\n",
    "\n",
    "with tqdm.tqdm(total=ntest) as pbar: \n",
    "    for ii, (thetao_i, sxo_i, xo_i) in enumerate(zip(params_test, sx_test_zt, x_test)): \n",
    "\n",
    "        # predict the posterior\n",
    "        post_hat_zt = model_params_mdn.predict(sxo_i.reshape(1, -1))\n",
    "        dd = post_hat_zt.get_dd_object()\n",
    "        # transform back to original parameter range\n",
    "        post_hat = post_hat_zt.ztrans_inv(param_norm[0], param_norm[1])\n",
    "        marginals_hat = post_hat.get_marginals()\n",
    "        \n",
    "        # the exact posterior \n",
    "        post_exact = NBExactPosterior(xo_i, prior_k, prior_theta)\n",
    "        post_exact.calculat_exact_posterior(theta_o=thetao_i, verbose=False, prec=1e-6, n_samples=300)\n",
    "        \n",
    "        marginals_exact = post_exact.get_marginals()\n",
    "        ps.append(post_exact)\n",
    "        ps_hat.append(post_hat)\n",
    "        ms.append(marginals_exact)\n",
    "        ms_hat.append(marginals_hat)\n",
    "       \n",
    "        try:\n",
    "            pbar.update()\n",
    "            # perform check for marginals         \n",
    "            for vi, (m, mhat, th) in enumerate(zip(marginals_exact, marginals_hat, thetao_i)):             \n",
    "                # means and std \n",
    "                # generate samples for estimating the mean and std\n",
    "                ss = m.gen(10000)\n",
    "                mus_exact[vi, ii], stds_exact[vi, ii] = m.mean, m.std\n",
    "                mus_hat[vi, ii], stds_hat[vi, ii] = mhat.mean, mhat.std\n",
    "\n",
    "                # quantiles \n",
    "                qis[vi, ii] = m.cdf(th)\n",
    "                qis_hat[vi, ii] = mhat.get_quantile(th)\n",
    "\n",
    "                # credible intervals\n",
    "                marginal_ci_counts[vi, ii, :] = m.get_credible_interval_counts(th, credible_intervals)\n",
    "                marginal_ci_counts_hat[vi, ii, :] = mhat.get_credible_interval_counts(th, credible_intervals)\n",
    "\n",
    "                # DKL \n",
    "                baseline = calculate_dkl_1D_scipy(m.pdf_array, priors[vi].pdf(m.support))\n",
    "                (dkl, err) = calculate_dkl_monte_carlo(np.array(ss), m.pdf, mhat.eval_numpy)\n",
    "                dkl_ratios[vi, ii] = dkl / baseline\n",
    "\n",
    "            # perform checks for joint \n",
    "            vi = 2\n",
    "\n",
    "            # quantiles \n",
    "            qis[vi, ii] = post_exact.cdf(thetao_i.reshape(1, -1))\n",
    "            qis_hat[vi, ii] = post_hat.get_quantile(thetao_i.reshape(1, -1))\n",
    "\n",
    "            # DKL \n",
    "            post_samples = post_exact.gen(20000)\n",
    "            (baseline, err) = calculate_dkl_monte_carlo(post_samples, post_exact.pdf, joint_prior.pdf)\n",
    "            (dkl, err) = calculate_dkl_monte_carlo(post_samples, post_exact.pdf, post_hat.eval_numpy)\n",
    "            (dkl_ratios[vi, ii], err) = dkl / baseline\n",
    "\n",
    "            # covariances\n",
    "            covariances.append(post_exact.cov)\n",
    "            covariances_hat.append(post_hat.get_covariance_matrix())     \n",
    "        except: \n",
    "            fails.append(ii)\n",
    "            continue\n",
    "fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save posterior checks results \n",
    "result_dict = dict(qis=qis, qis_hat=qis_hat, dkl_ratios=dkl_ratios,\n",
    "                  marginal_ci_counts=marginal_ci_counts, \n",
    "                  marginal_ci_counts_hat=marginal_ci_counts_hat, \n",
    "                  fails=fails, \n",
    "                  ntest=ntest, \n",
    "                  mus_exact=mus_exact, mus_hat=mus_hat, \n",
    "                  stds_exact=stds_exact, stds_hat=stds_hat, \n",
    "                  credible_intervals=credible_intervals, \n",
    "                  covariances=covariances, \n",
    "                  covariances_hat=covariances_hat, \n",
    "                  params_test=params_test, \n",
    "                  sx_test_zt=sx_test_zt, \n",
    "                  x_test=x_test, \n",
    "                  ps=ps, \n",
    "                  p_hats=ps_hat)\n",
    "\n",
    "fn = time_stamp + 'posterior_checks_results_NB_ntrain{}_ns{}_ntest{}'.format(ntrain, sample_size, ntest) + '.p'\n",
    "with open(os.path.join('../data', fn), 'wb') as outfile: \n",
    "    pickle.dump(result_dict, outfile, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mackelab]",
   "language": "python",
   "name": "conda-env-mackelab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
