{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1: \n",
    "- ### learn the model comparison between a K and a Na channel model. \n",
    "- ### predict a model idx given observed data \n",
    "- ### learn the posterior over model parameters of the predicted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import delfi.distribution as dd\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from delfi.generator import Default\n",
    "from delfi.utils.viz import plot_pdf\n",
    "from delfi.distribution.mixture import MoG\n",
    "\n",
    "from lfimodels.channelomics.ChannelSingle import ChannelSingle\n",
    "from lfimodels.channelomics.ChannelSuper import ChannelSuper\n",
    "from lfimodels.channelomics.ChannelStats import ChannelStats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set \"k\" or \"na\" as underlying ground truth model, generate observed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_type = 'k'\n",
    "result_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT = {'k': np.array([9, 25, 0.02, 0.002]),\n",
    "      'na': np.array([-35, 9, 0.182, 0.124, -50, -75, 5, -65, 6.2, 0.0091, 0.024])}\n",
    "\n",
    "LP = {'k': ['qa','tha','Ra','Rb'],\n",
    "      'na': ['tha','qa','Ra','Rb','thi1','thi2','qi','thinf','qinf','Rg','Rd']}\n",
    "\n",
    "E_channel = {'k': -86.7, 'na': 50}\n",
    "fact_inward = {'k': 1, 'na': -1}\n",
    "\n",
    "gt = GT[channel_type]\n",
    "cython = True\n",
    "third_exp_model = True\n",
    "\n",
    "n_params = len(gt)\n",
    "labels_params = LP[channel_type]\n",
    "prior_lims = np.sort(np.concatenate((0.2 * gt.reshape(-1,1), 1.3 * gt.reshape(-1,1)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate observed data\n",
    "n_params_obs = len(gt)\n",
    "m_obs = ChannelSingle(channel_type=channel_type, n_params=n_params_obs, cython=cython)\n",
    "s = ChannelStats(channel_type=channel_type)\n",
    "\n",
    "xo = m_obs.gen(gt.reshape(1,-1))\n",
    "xo_stats = s.calc(xo[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and split test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'training_data_k_na_N100000seed1.p'\n",
    "folder = '../data/'\n",
    "fullpath = os.path.join(folder, filename)\n",
    "\n",
    "with open(fullpath, 'rb') as f: \n",
    "    result_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_k, sx_k, gt_k, prior_lims_k, params_na, sx_na, gt_na, prior_lims_na, seed, n_samples, cython = result_dict.values()\n",
    "\n",
    "ntest = 1000\n",
    "n, n_stats = sx_na.shape\n",
    "ntrain = n - ntest\n",
    "\n",
    "# shuffle and set up model index target vector \n",
    "sx = np.vstack((sx_k[:ntrain, :], sx_na[:ntrain, :]))\n",
    "sx_test = np.vstack((sx_k[ntrain:, :], sx_na[ntrain:, :]))\n",
    "\n",
    "# define model indices\n",
    "m = np.hstack((np.zeros(ntrain), np.ones(ntrain))).squeeze().astype(int).tolist()\n",
    "m_test = np.hstack((np.zeros(ntest), np.ones(ntest))).squeeze().astype(int).tolist()\n",
    "\n",
    "# normalize data \n",
    "sx_zt, data_norm = normalize(sx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set up the NN and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_models = ClassificationMDN(n_input=n_stats, n_hidden_units=4, n_hidden_layers=1)\n",
    "optimizer = torch.optim.Adam(model_models.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_models, optimizer, verbose=True, classification=True)\n",
    "\n",
    "n_epochs = 10 \n",
    "n_minibatch = int(ntrain / 100)\n",
    "\n",
    "# train with training data\n",
    "loss_trace = trainer.train(sx_zt, m, n_epochs=n_epochs, n_minibatch=n_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 3))\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict underlying model given observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "# normalize using training data normalization \n",
    "sx_obs_zt, _ = normalize(xo_stats, data_norm)\n",
    "\n",
    "p_vec = model_models.predict(sx_obs_zt)\n",
    "print('True model: {}'.format(channel_type))\n",
    "print('P(K | sx) = {:.2f}'.format(p_vec[0]))\n",
    "print('P(Na | sx) = {:.2f}'.format(p_vec[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test data and calculate test cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_test_zt, data_norm = normalize(sx_test, data_norm)\n",
    "p = model_models.predict(sx_test_zt)[:, 1]\n",
    "y = np.array(m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = -(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "print('Cross entropy test loss: {:.4f}'.format(cel.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idx_posterior_dict = dict(sx_train=sx, sx_test=sx_test, mtrain=m, mtest=m_test, data_norm=data_norm, \n",
    "                           sx_obs=xo_stats, model_idx_mdn=model_models, prior_lims_k=prior_lims_k, \n",
    "                           prior_lims_na=prior_lims_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the predicted underlying model we can learn the posterior of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_model_idx = np.argmax(p_vec)\n",
    "predicted_channel_type = ['k', 'na'][predicted_model_idx]\n",
    "gt = GT[channel_type]\n",
    "n_params_pred = len(gt)\n",
    "\n",
    "# get corresponding data \n",
    "ntest = 1000\n",
    "ntrain = result_dict['sx_' + predicted_channel_type].shape[0] - ntest\n",
    "sx_pred = result_dict['sx_' + predicted_channel_type][:ntrain, ]\n",
    "sx_pred_test = result_dict['sx_' + predicted_channel_type][ntrain:]\n",
    "\n",
    "params_pred = result_dict['params_' + predicted_channel_type][:ntrain, ]\n",
    "params_pred_test = result_dict['params_' + predicted_channel_type][ntrain:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_channel_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG \n",
    "model_params_mdn = MultivariateMogMDN(ndim_input=n_stats, ndim_output=n_params_pred, n_hidden_layers=2, \n",
    "                                      n_hidden_units=10, n_components=1)\n",
    "optimizer = torch.optim.Adam(model_params_mdn.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_params_mdn, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize training data: k data only \n",
    "sx_normed, data_norm = normalize(sx_pred)\n",
    "\n",
    "# normalize the parameters as well \n",
    "params_ztrans, prior_norm = normalize(params_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_trace = trainer.train(sx_normed, params_ztrans, n_epochs=200, n_minibatch=int(sx_pred.shape[0] / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 3))\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the observed data with training norm \n",
    "sx_obs, data_norm = normalize(xo_stats.squeeze(), data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict MoG parameters given observed data \n",
    "mog_posterior_pytorch = model_params_mdn.predict(sx_obs.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learned MoG posterior using delfi distribution class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define as delfi distribution\n",
    "mog_posterior_delfi_zt = mog_posterior_pytorch.get_dd_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform back to prior ranges \n",
    "mog_posterior_delfi = mog_posterior_delfi_zt.ztrans_inv(mean=prior_norm[0], std=prior_norm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_pdf(mog_posterior_delfi, lims=prior_lims, figsize=(18, 10), ticks=True, \n",
    "                   labels_params=LP[predicted_channel_type], gt=gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to quantify the performance: \n",
    "\n",
    "sensitvity analysis? \n",
    "\n",
    "predictive checking? \n",
    "\n",
    "painfree estimate Macke, Wichmann \n",
    "coverage analysis \n",
    "Prangle paper, coverage property \n",
    "\n",
    "compare again SCM ABC, DIYABC, just sample a lot of data\n",
    "\n",
    "it works on toy, why to do it? some real world examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_posterior_dict = dict(xo=xo, sxo=xo_stats, parameter_mdn=model_params_mdn, gt=GT, labels=LP, \n",
    "                           sx_train=sx_pred, sx_test=sx_pred_test, \n",
    "                           params_pred=params_pred, params_pred_test=params_pred_test, \n",
    "                           predicted_channel_type=predicted_channel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict(model_idx_posterior=model_idx_posterior_dict, \n",
    "                   parameter_posterior=parameter_posterior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'learned_posteriors_pipeline1_NaK_ntrain{}.p'.format(sx.shape[0])\n",
    "folder = '../data/'\n",
    "with open(os.path.join(folder, filename), 'wb') as outfile: \n",
    "    pickle.dump(result_dict, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
