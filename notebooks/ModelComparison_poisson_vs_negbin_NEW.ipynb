{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "from random import shuffle\n",
    "from scipy.stats import gamma, beta, nbinom, poisson\n",
    "from scipy.special import gammaln, betaln\n",
    "from torch.autograd import Variable\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 17\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 16\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Poisson data, compare Poisson vs. negBin model\n",
    "\n",
    "I will train a MDN to approximate the posterior over model indices. As models I use a Poisson with Gamma prior and a negative Binomial sampled from a Poisson-Gamma mixture with Gamma priors on the Gamma shape and scale parameters. \n",
    "\n",
    "As training data I will generate a large data set containing samples from both models generated with the corresponding priors. The MDN gets the mean and variance of the data set as input and outputs a probability vector over models. \n",
    "\n",
    "### Controlling difficulty via over / under dispersion \n",
    "The prior parameters of the conjugate Gamma prior of the Poisson model and the two Gamma priors of the NB model are chosen such that on average the data sets generated by the models have idenitcal sample means. The difficulty therefore arises from the amount of overdispersion in the NB model: The model comparison problem becomes easier to solve as the variance in the samples form the NB model is larger than the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "n_samples = 10000\n",
    "seed = None\n",
    "\n",
    "# set prior parameters \n",
    "# set the shape or scale of the Gamma prior for the Poisson model\n",
    "k1 = 9.0 \n",
    "# set the shape and scale of the prior on the shape of the Gamma for the mixture to be broad \n",
    "theta2 = 2.0\n",
    "k2 = 5.\n",
    "# set the shape and scale of the prior on the scale of the Gamma for the mixture to be small \n",
    "# this will make the variance and could be the tuning point of the amount of overdispersion / difficulty\n",
    "theta3 = 1.0 \n",
    "k3 = 1\n",
    "\n",
    "# then the scale of the Gamma prior for the Poisson is given by \n",
    "theta1 = (k2 * theta2 * k3 * theta3) / k1\n",
    "print(theta1, k1)\n",
    "\n",
    "# get analytical means \n",
    "mean_ana_poi = k1 * theta1\n",
    "mean_ana_nb = k2 * k3 * theta2 * theta3\n",
    "\n",
    "# set the priors \n",
    "prior1 = scipy.stats.gamma(a=k1, scale=theta1)\n",
    "prior2 = scipy.stats.gamma(a=k2, scale=theta2)\n",
    "prior3 = scipy.stats.gamma(a=k3, scale=theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a large data set for training \n",
    "\n",
    "X = []\n",
    "thetas = []\n",
    "m = []\n",
    "\n",
    "for sample_idx in range(n_samples): \n",
    "    \n",
    "    # sample model index \n",
    "    m.append(int(np.round(np.random.rand())))\n",
    "    \n",
    "    if m[sample_idx] == 0: \n",
    "        # sample poisson \n",
    "        theta, x = sample_poisson(prior1, 1, sample_size)\n",
    "    if m[sample_idx] == 1: \n",
    "        # sample poisson \n",
    "        theta, x = sample_poisson_gamma_mixture(prior2, prior3, 1, sample_size)\n",
    "\n",
    "    # calculate mean and var as summary stats \n",
    "    X.append([np.mean(x), np.var(x)])\n",
    "    thetas.append(theta)\n",
    "    \n",
    "X = np.array(X)\n",
    "#thetas = np.array(thetas)\n",
    "#m = np.array(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network for fitting the model posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDN_psi(nn.Module):\n",
    "    \n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=5, n_components=1):\n",
    "        super(MDN_psi, self).__init__()\n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.m_out = nn.Linear(n_hidden, ndim_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        out_m = self.m_out(act)\n",
    "        return out_m\n",
    "    \n",
    "def train_psi(X, Y, model, optim, lossfun, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))        \n",
    "            y_var = Variable(torch.LongTensor(y_batch)).view(n_minibatch)\n",
    "            \n",
    "            (out_act) = model(x_var)\n",
    "            loss = lossfun(out_act, y_var)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data[0])\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "    \n",
    "    return model, optim, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a large data set of triplets (m, theta, sx)\n",
    "\n",
    "Then separate it into sets for model 1 and model 2 and train the phi networks separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize \n",
    "X, norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2\n",
    "model = MDN_psi(ndim_input=n_inputs, n_hidden=10)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "model_psi, optim_psi, losses = train_psi(X, m, model, optim, lossfun, n_epochs=500, n_minibatch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize input-output function of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.logspace(-1, 2, 100), np.logspace(-1, 3, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoi_mat = np.zeros((100, 100))\n",
    "softmax = nn.Softmax(dim=0)\n",
    "\n",
    "for i in range(x.shape[0]): \n",
    "    for j in range(x.shape[0]): \n",
    "        stats_o, norm = normalize(np.array([x[i, j], y[i, j]]), norm)        \n",
    "        (out_act) = model(Variable(torch.Tensor(stats_o)))\n",
    "    \n",
    "        # in this vector, index 0 is Poi, index 1 is NB\n",
    "        posterior_probs = softmax(out_act).data.numpy()\n",
    "        ppoi_mat[i, j] = posterior_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ppoi_mat, origin='lower', extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Variance')\n",
    "plt.colorbar(label='P(Poisson)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare network predictions to analytical posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
