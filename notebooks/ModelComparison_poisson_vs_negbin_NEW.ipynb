{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "from random import shuffle\n",
    "from scipy.stats import gamma, beta, nbinom, poisson\n",
    "from scipy.special import gammaln, betaln\n",
    "from torch.autograd import Variable\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 17\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 16\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Poisson data, compare Poisson vs. negBin model\n",
    "\n",
    "I will train a MDN to approximate the posterior over model indices. As models I use a Poisson with Gamma prior and a negative Binomial sampled from a Poisson-Gamma mixture with Gamma priors on the Gamma shape and scale parameters. \n",
    "\n",
    "As training data I will generate a large data set containing samples from both models generated with the corresponding priors. The MDN gets the mean and variance of the data set as input and outputs a probability vector over models. \n",
    "\n",
    "### Controlling difficulty via over / under dispersion \n",
    "The prior parameters of the conjugate Gamma prior of the Poisson model and the two Gamma priors of the NB model are chosen such that on average the data sets generated by the models have idenitcal sample means. The difficulty therefore arises from the amount of overdispersion in the NB model: The model comparison problem becomes easier to solve as the variance in the samples form the NB model is larger than the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1111111111111112 9.0\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "n_samples = 100000\n",
    "\n",
    "# set RNG\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "time_stamp = time.strftime('%Y%m%d%H%M_')\n",
    "figure_folder = '../figures/'\n",
    "\n",
    "# set prior parameters \n",
    "# set the shape or scale of the Gamma prior for the Poisson model\n",
    "k1 = 9.0 \n",
    "# set the shape and scale of the prior on the shape of the Gamma for the mixture to be broad \n",
    "theta2 = 2.0\n",
    "k2 = 5.\n",
    "# set the shape and scale of the prior on the scale of the Gamma for the mixture to be small \n",
    "# this will make the variance and could be the tuning point of the amount of overdispersion / difficulty\n",
    "theta3 = 1.0 \n",
    "k3 = 1\n",
    "\n",
    "# then the scale of the Gamma prior for the Poisson is given by \n",
    "theta1 = (k2 * theta2 * k3 * theta3) / k1\n",
    "print(theta1, k1)\n",
    "\n",
    "# get analytical means \n",
    "mean_ana_poi = k1 * theta1\n",
    "mean_ana_nb = k2 * k3 * theta2 * theta3\n",
    "\n",
    "# set the priors \n",
    "prior_lam = scipy.stats.gamma(a=k1, scale=theta1)\n",
    "prior_k = scipy.stats.gamma(a=k2, scale=theta2)\n",
    "prior_theta = scipy.stats.gamma(a=k3, scale=theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a large data set for training \n",
    "\n",
    "X = []\n",
    "thetas = []\n",
    "m = []\n",
    "\n",
    "for sample_idx in range(n_samples): \n",
    "    \n",
    "    # sample model index \n",
    "    m.append(int(np.round(np.random.rand())))\n",
    "    \n",
    "    if m[sample_idx] == 0: \n",
    "        # sample poisson \n",
    "        theta, x = sample_poisson(prior_lam, 1, sample_size)\n",
    "    if m[sample_idx] == 1: \n",
    "        # sample poisson \n",
    "        theta, x, lambs = sample_poisson_gamma_mixture(prior_k, prior_theta, 1, sample_size)\n",
    "\n",
    "    # calculate mean and var as summary stats \n",
    "    X.append([np.mean(x), np.var(x)])\n",
    "    thetas.append(theta)\n",
    "    \n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network for fitting the model posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDN_psi(nn.Module):\n",
    "    \n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=5, n_components=1):\n",
    "        super(MDN_psi, self).__init__()\n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.m_out = nn.Linear(n_hidden, ndim_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        out_m = self.m_out(act)\n",
    "        return out_m\n",
    "    \n",
    "def train_psi(X, Y, model, optim, lossfun, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))        \n",
    "            y_var = Variable(torch.LongTensor(y_batch)).view(n_minibatch)\n",
    "            \n",
    "            (out_act) = model(x_var)\n",
    "            loss = lossfun(out_act, y_var)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data[0])\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "    \n",
    "    return model, optim, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a large data set of triplets (m, theta, sx)\n",
    "\n",
    "Then separate it into sets for model 1 and model 2 and train the phi networks separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize \n",
    "X, norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2\n",
    "model = MDN_psi(ndim_input=n_inputs, n_hidden=10)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "model_psi, optim_psi, losses = train_psi(X, m, model, optim, lossfun, n_epochs=200, n_minibatch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize input-output function of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.linspace(0, 100, 100), np.linspace(0, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppoi_mat = np.zeros((100, 100))\n",
    "softmax = nn.Softmax(dim=0)\n",
    "\n",
    "for i in range(x.shape[0]): \n",
    "    for j in range(x.shape[0]): \n",
    "        stats_o, norm = normalize(np.array([x[i, j], y[i, j]]), norm)        \n",
    "        (out_act) = model(Variable(torch.Tensor(stats_o)))\n",
    "    \n",
    "        # in this vector, index 0 is Poi, index 1 is NB\n",
    "        posterior_probs = softmax(out_act).data.numpy()\n",
    "        ppoi_mat[i, j] = posterior_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ppoi_mat, origin='lower', extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Variance')\n",
    "plt.colorbar(label='P(Poisson)');\n",
    "plt.tight_layout()\n",
    "save_figure(filename='network_visualization_2d_M{}N{}'.format(sample_size, n_samples), \n",
    "            folder=figure_folder, \n",
    "            time_stamp=time_stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate test data from both models\n",
    "testX = []\n",
    "testSX = []\n",
    "thetas = []\n",
    "m = []\n",
    "n_samples = 10\n",
    "sample_size = sample_size\n",
    "markers = []\n",
    "lambs = []\n",
    "\n",
    "for sample_idx in range(n_samples): \n",
    "    \n",
    "    # sample model index \n",
    "    m.append(int(np.round(np.random.rand())))\n",
    "    \n",
    "    if m[sample_idx] == 0: \n",
    "        # sample poisson \n",
    "        theta, x = sample_poisson(prior_lam, 1, sample_size)\n",
    "        markers.append('x')\n",
    "    if m[sample_idx] == 1: \n",
    "        # sample poisson \n",
    "        theta, x, ls = sample_poisson_gamma_mixture(prior_k, prior_theta, 1, sample_size)\n",
    "        markers.append('o')\n",
    "        lambs.append(ls)\n",
    "\n",
    "    # calculate mean and var as summary stats \n",
    "    testX.append(x)\n",
    "    testSX.append([np.mean(x), np.var(x)])\n",
    "    thetas.append(theta)\n",
    "    \n",
    "testX = np.array(testX).squeeze()\n",
    "lambs = np.array(lambs)\n",
    "testSX = np.array(testSX)\n",
    "testm = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ppoi = []\n",
    "for xtest in testSX: \n",
    "    stats_o, norm = normalize(xtest, norm)        \n",
    "    (out_act) = model(Variable(torch.Tensor(stats_o)))\n",
    "\n",
    "    # in this vector, index 0 is Poi, index 1 is NB\n",
    "    test_ppoi.append(softmax(out_act).data.numpy()[0])    \n",
    "\n",
    "test_ppoi = np.array(test_ppoi)\n",
    "test_ppoi[test_ppoi == 1.] = 0.999\n",
    "lbf_pred = np.log(test_ppoi) - np.log(1 - test_ppoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_mask = np.array(testm) == 0\n",
    "nb_mask = np.logical_not(poi_mask)\n",
    "x, y = np.meshgrid(np.linspace(0, 100, 100), np.linspace(0, 100, 100))\n",
    "\n",
    "# color norm \n",
    "colornorm = mpl.colors.Normalize(vmin=test_ppoi.min(), vmax=test_ppoi.max())\n",
    "\n",
    "plt.scatter(x=testSX[poi_mask, 0], y=testSX[poi_mask, 1], \n",
    "            cmap='viridis', c=test_ppoi[poi_mask], marker='x', \n",
    "           label='Poisson', norm=colornorm)\n",
    "plt.scatter(x=testSX[nb_mask, 0], y=testSX[nb_mask, 1], \n",
    "            cmap='viridis', c=test_ppoi[nb_mask], marker='^', label='NegBin', norm=colornorm)\n",
    "\n",
    "plt.colorbar(label='p(poisson)', orientation='vertical')\n",
    "plt.xlabel('mean')\n",
    "plt.ylabel('variance')\n",
    "#plt.contour(x, y, ppoi_mat)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare network predictions to analytical posterior probabilities\n",
    "\n",
    "The network outputs a vector of posterior model probabilities. Ideally, we have the same values in analytical form. Up to now we just derived the analytical form of the individual model evidences and from that the analytical Bayes factor. This we can compare to the predicted posterior probability because we have a uniform model prior. With a uniform model prior $(p(M_1) = p(M_2))$ the posterior ratio is the same as the evidence ration (Bayes factor):\n",
    "\n",
    "\\begin{align}\n",
    "BayesFactor = \\frac{p(D | M_1)}{p(D | M_2)} = \\frac{p(M_1 | D) p(M_1)}{p(M_2 | D) p(M_2)} = \\frac{p(M_1 | D)}{p(M_2 | D)}\n",
    "\\end{align}\n",
    "\n",
    "Thus, we can just calculate the individual model evidences and compare their ratio to the ratio of the output of the network. \n",
    "\n",
    "Alternatively, we can derive the analytical posterior probabilities for each model and compare it directly to the probabilities predicted by the network. \n",
    "\n",
    "\\begin{align}\n",
    "p(M_i | D) &= \\frac{p(D|M_i)p(M_i)}{p(D)} \\\\\n",
    "&= \\frac{p(D|M_i)p(M_i)}{\\sum_j^{|M|} p(D | M_j)} \\\\ \n",
    "&= \\frac{\\int p(D|\\theta)p(\\theta|M_i)d\\theta \\; p(M_i)}{\\sum_j^{|M|} \\int p(D|\\theta)p(\\theta|M_j)d\\theta}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a grid of values around the priors \n",
    "# take grid over the whole range of the priors\n",
    "k_start = scipy.stats.gamma.ppf(1e-8, a=k2)\n",
    "k_end = scipy.stats.gamma.ppf(1 - 1e-8, a=k2)\n",
    "\n",
    "theta_start = scipy.stats.gamma.ppf(1e-8, a=k3)\n",
    "theta_end = scipy.stats.gamma.ppf(1 - 1e-8, a=k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nb evidence for every test set \n",
    "nb_evidences = []\n",
    "poi_evidences = []\n",
    "\n",
    "for ii, (x, mi) in enumerate(zip(testX, testm)):\n",
    "    (lquad, err) = scipy.integrate.dblquad(func=nb_evidence_integrant_direct,\n",
    "                                           a=theta_start / (1 + theta_start),\n",
    "                                           b=theta_end / (1 + theta_end),\n",
    "                                           gfun=lambda x: k_start, hfun=lambda x: k_end, \n",
    "                                           args=[x, prior_k, prior_theta])\n",
    "    print(mi, 'NB', np.log(lquad))\n",
    "    nb_evidences.append(np.log(lquad))\n",
    "\n",
    "    le = poisson_evidence(x, k=k1, theta=theta1, log=True)\n",
    "    print(mi, 'Poi', le)\n",
    "    poi_evidences.append(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_evidences = np.array(poi_evidences)\n",
    "nb_evidences = np.array(nb_evidences)\n",
    "lbf_ana = poi_evidences - nb_evidences\n",
    "ppoi_ana = calculate_pprob_from_evidences(np.exp(poi_evidences), np.exp(nb_evidences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lbf_pred, 'o-', label='predicted')\n",
    "plt.plot(lbf_ana, 'o-', label='analytical')\n",
    "plt.xlabel('test data set index')\n",
    "plt.ylabel('log bayes factor')\n",
    "plt.legend();\n",
    "plt.tight_layout()\n",
    "save_figure(filename='bayesfactor_prediction_evaluation_M{}N{}'.format(sample_size, n_samples), \n",
    "            folder=figure_folder, \n",
    "            time_stamp=time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_ppoi, 'o-', label='predicted')\n",
    "plt.plot(ppoi_ana, 'o-', label='analytical')\n",
    "plt.xlabel('test data set index')\n",
    "plt.ylabel(r'$Pr(M_{poi} | X)$')\n",
    "plt.legend();\n",
    "plt.tight_layout()\n",
    "save_figure(filename='posterior_prediction_evaluation_M{}N{}'.format(sample_size, n_samples), \n",
    "            folder=figure_folder, \n",
    "            time_stamp=time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
