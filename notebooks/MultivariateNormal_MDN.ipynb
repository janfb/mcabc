{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from scipy.stats import beta\n",
    "import scipy.special\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a MDN for approximating a multidimensional Gaussian\n",
    "\n",
    "It takes as input the data $x$ **and** the model index $m$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=10, n_components=1):\n",
    "        super(MDN, self).__init__()\n",
    "        \n",
    "        self.ndims = ndim_output \n",
    "        # the number of entries in the upper triangular Choleski transform matrix of the precision matrix \n",
    "        self.utriu_entries = int(self.ndims * (self.ndims - 1) / 2) + self.ndims\n",
    "\n",
    "        # input layer \n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        # activation \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # output layer the mean estimates \n",
    "        self.mu_out = nn.Linear(n_hidden, ndim_output)\n",
    "\n",
    "        # output layer to precision estimates \n",
    "        # the upper triangular matrix for D-dim Gaussian has m = (D**2 + D) / 2 entries \n",
    "        # this should be a m-vector for every component. currently it is just a scalar for every component. \n",
    "        # or it could be a long vector of length m * k, i.e, all the k vector stacked. \n",
    "        self.U_out = nn.Linear(n_hidden, self.utriu_entries * n_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        \n",
    "        out_mu = self.mu_out(act)\n",
    "\n",
    "        # get activate of upper triangle U vector\n",
    "        U_vec = self.U_out(act)\n",
    "        # prelocate U matrix \n",
    "        \n",
    "        U_mat = Variable(torch.zeros(batch_size, self.ndims, self.ndims))\n",
    "        \n",
    "        # assign vector to upper triangle of U \n",
    "        (idx1, idx2) = np.triu_indices(self.ndims)\n",
    "        U_mat[:, idx1, idx2] = U_vec\n",
    "        # apply exponential to get positive diagonal\n",
    "        (idx1, idx2) = np.diag_indices(self.ndims)\n",
    "        U_mat[:, idx1, idx2] = torch.exp(U_mat[:, idx1, idx2])\n",
    "\n",
    "        return (out_mu, U_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the loss evaluates model (MoG) with the given data (y) and takes the log loss\n",
    "def mdn_loss_function(y, mu, U):\n",
    "    \n",
    "#    result = ND_gauss_pdf(y, mu, U, log=True)\n",
    "    result = multivariate_normal_pdf(y, mu, U, log=True)    \n",
    "    \n",
    "    result = torch.mean(result)  # mean over batch\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_2DGaussian_dataset(n_samples, sample_size, prior, data_cov=None):\n",
    "\n",
    "    X = []\n",
    "    thetas = []\n",
    "    \n",
    "    if data_cov is None: \n",
    "        data_cov = .5 * np.eye(2)\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        # sample from the prior \n",
    "        theta = prior.rvs()\n",
    "\n",
    "        # generate samples with mean from prior and unit variance \n",
    "        x = scipy.stats.multivariate_normal.rvs(mean=theta, cov=data_cov, size=sample_size).reshape(sample_size, 2)\n",
    "        \n",
    "        sx = stats_ND_Gaussian(x)\n",
    "\n",
    "        # as data we append the summary stats\n",
    "        X.append(sx) \n",
    "        thetas.append([theta])   \n",
    "    \n",
    "    return np.array(X).squeeze(), np.array(thetas).squeeze()\n",
    "\n",
    "def stats_ND_Gaussian(x): \n",
    "    \"\"\"\n",
    "    Calculate the sufficient statistics of a multivariate Gaussian sample x\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(x, axis=0).astype(float)])\n",
    "\n",
    "def batch_generator(dataset, batch_size=5):\n",
    "    shuffle(dataset)\n",
    "    N_full_batches = len(dataset) // batch_size\n",
    "    for i in range(N_full_batches):\n",
    "        idx_from = batch_size * i\n",
    "        idx_to = batch_size * (i + 1)\n",
    "        xs, ys = zip(*[(x, y) for x, y in dataset[idx_from:idx_to]])\n",
    "        yield xs, ys\n",
    "        \n",
    "def train(X, Y, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))\n",
    "            y_var = Variable(torch.Tensor(y_batch))\n",
    "                        \n",
    "            (out_mu, out_U) = model(x_var)\n",
    "            loss = mdn_loss_function(y_var, out_mu, out_U)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data.numpy())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "            \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MDN(ndim_input=2, n_components=1)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "sample_size = 10\n",
    "    \n",
    "# prior on the mean\n",
    "data_cov = 0.5 * np.eye(2)\n",
    "prior = scipy.stats.multivariate_normal(mean=[0., 0.], cov=2. * np.eye(2))\n",
    "\n",
    "\n",
    "X, Y = generate_2DGaussian_dataset(n_samples, sample_size, prior, data_cov)\n",
    "X, norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train(X, Y, n_epochs=50, n_minibatch=100)\n",
    "plt.plot(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# observe data \n",
    "true_mu = [-.5, .1]\n",
    "true_cov = data_cov\n",
    "xo = scipy.stats.multivariate_normal.rvs(mean=true_mu, cov=true_cov, size=sample_size).reshape(sample_size, 2)\n",
    "# gets stats \n",
    "statso = stats_ND_Gaussian(xo)\n",
    "# normalize \n",
    "statso, norm = normalize(statso, norm)\n",
    "# cast to torch \n",
    "statso = Variable(torch.Tensor(statso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(211)\n",
    "plt.hist2d(X[:, 0], X[:, 1])\n",
    "plt.title('Samples generated by the model: 2D Gaussian with prior on $\\mu$');\n",
    "plt.colorbar();\n",
    "plt.subplot(212)\n",
    "plt.title('observed data')\n",
    "plt.hist2d(xo[:, 0], xo[:, 1])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with observed data \n",
    "(out_mu, out_U) = model(statso.view(1, 2))\n",
    "Sin = torch.mm(torch.transpose(out_U.view(2, 2), 0, 1), out_U.view(2, 2))\n",
    "\n",
    "# convert to numpy \n",
    "mean = out_mu.data.numpy().squeeze()\n",
    "cov = torch.inverse(Sin).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posterior = scipy.stats.multivariate_normal(mean=mean, cov=cov)\n",
    "# get analystical posterior \n",
    "post_mean, post_cov = calculate_multivariate_normal_mu_posterior(xo, data_cov, sample_size, \n",
    "                                                                prior.mean, prior.cov)\n",
    "postana = scipy.stats.multivariate_normal(mean=post_mean, cov=post_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_multivariate_normal_mu_posterior(X, sigma, N, mu_0, sigma_0): \n",
    "    sigma_N = np.linalg.inv(np.linalg.inv(sigma_0) + N * np.linalg.inv(sigma))\n",
    "    mu_N = sigma_N.dot(N * np.linalg.inv(sigma).dot(X.mean(axis=0)) + np.linalg.inv(sigma_0).dot(mu_0))\n",
    "    \n",
    "    return mu_N, sigma_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1.\n",
    "x, y = np.mgrid[-r:r:.01, -r:r:.01]\n",
    "pos = np.dstack((x, y))\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.subplot(311)\n",
    "plt.contourf(x, y, prior.pdf(pos))\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro')\n",
    "plt.title('Prior')\n",
    "plt.subplot(312)\n",
    "plt.contourf(x, y, posterior.pdf(pos))\n",
    "plt.title('Posterior')\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro');\n",
    "plt.subplot(313)\n",
    "plt.contourf(x, y, postana.pdf(pos))\n",
    "plt.title('Analytical Posterior')\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
