{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the posterior \n",
    "\n",
    "The checks are used to validate the posterior of the parameter given the data that was learned. In this case we also know the ground truth, therefore, we have the possibility the validate the predicted posterior probabilities directly as well. \n",
    "\n",
    "For the validation we use a testing data set, $N_{test}$ number of parameters drawn from the prior, and simulated forward to give testing data $X_{test}$. \n",
    "\n",
    "### MSE to ground truth posterior \n",
    "To evaluate the posterior probabilities directly we just calculate the mean squared error between the predicted and the exact posterior probabilities. For every data sample in $X_{test}$ we calculate the squared error and then we take the mean over test samples. \n",
    "\n",
    "### Posterior quantiles\n",
    "To do the quantile checks, we perform the following steps: \n",
    "    - for every test sample obtain the posterior \n",
    "    - calculate the quantile of the corresponding test parameter in the posterior distribution \n",
    "\n",
    "This gives $N_{test}$ quantile values. According to the paper the distribution underlying these quantile is uniform if the posterior is accurate. A first visual test is therefore to plot the histogram of quantiles and check whether it is uniform. As a next step one could perform statistical tests to test the uniformity of the empirical quantile distribution. \n",
    "\n",
    "### Coverage analysis: credible intervals of the quantile distribution \n",
    "Alternatively, and somehow more Bayesian, is to check the credible intervals of the quantile distribution. To this end, we count the mass in different intervals around the mode of the distribution, or, assuming that the quantile distribution is uniform in $[0, 1]$, around 0. Then we compare this mass against the width of the interval, e.g., plot the interval against the counted mass. For a perfectly uniform distribution this would be a straight line because the mass is distributed uniformly so that it exactly matches the width of the interval. In summary: \n",
    "     - count the relative number of quantiles in intervals of $0.1, \\ldots, 0.5$ arond 0 of the quantile histogram\n",
    "     - plot the relative counts against the width of the interval\n",
    "     \n",
    "### Check $D_{KL}$ between true and predicted posterior \n",
    "One can calculate the DKL between the true and the predicted posterior and divide by the entropy of the true posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "import sys \n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import *\n",
    "from model_comparison.models import PoissonModel, NegativeBinomialModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpl_params = {'legend.fontsize': 15,\n",
    "              'legend.frameon': False,\n",
    "                      'axes.titlesize': 20,\n",
    "                      'axes.labelsize': 17,\n",
    "                      'xtick.labelsize': 12,\n",
    "                      'ytick.labelsize': 12,\n",
    "             'figure.figsize' : (18, 5)}\n",
    "\n",
    "mpl.rcParams.update(mpl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = '../data/'\n",
    "fn = '201803211432_toy_example_results_N100000M10_k220.0.p'\n",
    "time_stamp = fn[:fn.find('_')]\n",
    "\n",
    "with open(os.path.join(folder, fn), 'rb') as f: \n",
    "    d = pickle.load(f)\n",
    "    \n",
    "# set the seed for generating new test data \n",
    "seed = 5\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = d['model_post']\n",
    "d_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = d_model['model_models']\n",
    "x = d_model['x']\n",
    "sx = d_model['sx']\n",
    "xtest = d_model['xtest']\n",
    "mtest = d_model['mtest']\n",
    "ppoi_exact = d_model['ppoi_exact']\n",
    "sx_test = d_model['sx_test']\n",
    "sample_size = d_model['sample_size']\n",
    "\n",
    "training_norm = d_model['training_norm']\n",
    "k1, k2, k3 = d_model['k1'], d_model['k2'], d_model['k3']\n",
    "theta1, theta2, theta3 = d_model['theta1'], d_model['theta2'], d_model['theta3']\n",
    "\n",
    "# priors \n",
    "prior_lambda = scipy.stats.gamma(a=k1, scale=theta1)\n",
    "prior_k = scipy.stats.gamma(a=k2, scale=theta2)\n",
    "prior_theta = scipy.stats.gamma(a=k3, scale=theta3)\n",
    "\n",
    "model_poisson = PoissonModel(sample_size=sample_size, seed=seed)\n",
    "model_nb = NegativeBinomialModel(sample_size=sample_size, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First the discrete model idx posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize network and test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate grid of predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms, vs = np.meshgrid(np.linspace(0, 30, 100), np.linspace(0, 200, 100))\n",
    "# stack values to evaluate as vector in the model \n",
    "sx_vis = np.vstack((ms.flatten(), vs.flatten())).T\n",
    "# normalize \n",
    "sx_vis, training_norm = normalize(sx_vis, training_norm)\n",
    "\n",
    "\n",
    "# predict probs \n",
    "ppoi_vec = model.predict(sx_vis)\n",
    "# take poisson posterior prob and reshape to grid dimensions\n",
    "ppoi_vismat = ppoi_vec[:, 0].reshape(ms.shape[0], vs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot exact posterior probs on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 5))\n",
    "cnorm = mpl.colors.Normalize(vmin=ppoi_vismat.min(), vmax=ppoi_vismat.max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "plt.scatter(x=sx_test[:, 0], y=sx_test[:, 1], c=np.array(ppoi_exact), cmap=cmap, norm=cnorm, edgecolors='r', linewidths=0.3)\n",
    "\n",
    "plt.imshow(ppoi_vismat, origin='lower', aspect='auto', \n",
    "           norm=cnorm, cmap=cmap, extent=[ms.min(), ms.max(), vs.min(), vs.max()])\n",
    "\n",
    "plt.xlabel('Sample mean')\n",
    "plt.ylabel('Sample variance')\n",
    "plt.colorbar(label='P(Poisson | x)', pad=0.03)\n",
    "plt.legend(['Exact posterior probabilities'], frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_test_zt, training_norm = normalize(sx_test, training_norm)\n",
    "ppoi_hat = model.predict(sx_test_zt)[:, 0]\n",
    "mse = np.mean(np.abs(ppoi_hat - ppoi_exact))\n",
    "print('MAE', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(ppoi_exact, 'o-', label='exact')\n",
    "plt.plot(ppoi_hat, 'o-', label='predicted');\n",
    "plt.title('Test sample posterior probability')\n",
    "plt.legend()\n",
    "plt.xlabel('test sample idx')\n",
    "plt.ylabel('p(poi | sx)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantiles\n",
    "### first generate more test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntest = 1000\n",
    "\n",
    "params_poi = prior_lambda.rvs(size=int(ntest / 2))\n",
    "params_nb = np.vstack((prior_k.rvs(size=int(ntest / 2)), \n",
    "                       prior_theta.rvs(size=int(ntest / 2)))).T\n",
    "\n",
    "data_poi = model_poisson.gen(params_poi)\n",
    "data_nb = model_nb.gen(params_nb)\n",
    "\n",
    "x_test = np.vstack((data_poi, data_nb))\n",
    "m_test = np.hstack((np.zeros(data_poi.shape[0]), np.ones(data_nb.shape[0]))).squeeze().astype(int)\n",
    "sx_test = calculate_stats_toy_examples(x_test)\n",
    "# use training norm to normalize test data \n",
    "sx_test_zt, training_norm = normalize(sx_test, training_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qis = np.ones_like(np.array(m_test)).astype(float)\n",
    "# get posteriors \n",
    "posteriors = model.predict(sx_test_zt)\n",
    "# get probability for m=1\n",
    "p_m1s = posteriors[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate quantiles \n",
    "m1_mask = np.array(m_test) == 1\n",
    "qis[m1_mask] = 1. - p_m1s[m1_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(qis, bins='auto')\n",
    "plt.title('Posterior quantile histrogram')\n",
    "plt.xlabel('quantile')\n",
    "plt.ylabel('counts')\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the continuous model parameter posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_param_posterior = d['param_post']\n",
    "d_param_posterior.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_posterior_mdn = d_param_posterior['params_mdn']\n",
    "training_norm = d_param_posterior['training_norm']\n",
    "prior_norm = d_param_posterior['prior_norm']\n",
    "seed = d_param_posterior['seed']\n",
    "sample_size = d_param_posterior['sample_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample new test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntest = 1000\n",
    "prior_lambda = scipy.stats.gamma(a=k1, scale=theta1)\n",
    "params_test = prior_lambda.rvs(size=ntest)\n",
    "poisson_model = PoissonModel(sample_size=sample_size, seed=seed, n_workers=1)\n",
    "x_test = poisson_model.gen(params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sx_test = calculate_stats_toy_examples(x_test)\n",
    "sx_test_zt, _ = normalize(sx_test, training_norm)\n",
    "params_test_zt, _ = normalize(params_test, prior_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over test samples for \n",
    "- #### Quantiles \n",
    "- #### posterior mean differences \n",
    "- #### $D_{KL}$\n",
    "- #### credible intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qis = np.zeros(ntest)\n",
    "mus_hat = np.zeros_like(qis)\n",
    "mus_exact = np.zeros_like(qis)\n",
    "stds_hat = np.zeros_like(qis)\n",
    "stds_exact = np.zeros_like(qis)\n",
    "dkl_ratios = np.zeros_like(qis)\n",
    "credible_intervals = np.arange(0.05, 1., 0.05)\n",
    "success_counts = np.zeros_like(credible_intervals)\n",
    "\n",
    "# for every test sample \n",
    "with tqdm.tqdm(total=ntest) as pbar: \n",
    "    for ii, (thetao_i, sxo_i, xo_i) in enumerate(zip(params_test, sx_test_zt, x_test)): \n",
    "\n",
    "        tic = time.time()\n",
    "        # predict the posterior\n",
    "        post_hat = params_posterior_mdn.predict(sxo_i.reshape(1, -1))\n",
    "        # get dd object \n",
    "        post_hat_dd = post_hat.get_dd_object()\n",
    "        # transform to original parameter range \n",
    "        post_hat_or = post_hat_dd.ztrans_inv(prior_norm[0], prior_norm[1])\n",
    "\n",
    "        # get the mean \n",
    "        mus_hat[ii] = post_hat_or.mean\n",
    "        stds_hat[ii] = post_hat_or.std\n",
    "\n",
    "        # get the mean of the exact posterior \n",
    "        post_exact = poisson_model.get_exact_posterior(xo_i, k1, theta1)\n",
    "        mus_exact[ii] = post_exact.mean()    \n",
    "        stds_exact[ii] = post_exact.std()\n",
    "        # DKL ratio\n",
    "        # get mle gamma estimate \n",
    "        post_mle = poisson_model.get_mle_posterior(10000)\n",
    "        dkl_baseline = calculate_gamma_dkl(post_exact.kwds['a'], post_exact.kwds['scale'], \n",
    "                                          post_mle.kwds['a'], post_mle.kwds['scale'])\n",
    "        dkl_ratios[ii] = calculate_dkl(post_exact, post_hat_or) / dkl_baseline\n",
    "\n",
    "        # credible intervals success\n",
    "        # calculate samples \n",
    "        success_counts += calculate_credible_intervals_success(thetao_i, calculate_ppf_from_samples, credible_intervals, \n",
    "                                                               args=[post_hat_or.gen(10000)])\n",
    "        # get quantile of theta_o\n",
    "        thetao_i_zt, _ = normalize(thetao_i, prior_norm)\n",
    "        qis[ii] = post_hat.get_quantile(thetao_i_zt)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, sharey=False, figsize=(18, 5))\n",
    "axes = axes.reshape(1, -1)\n",
    "n, bins, p = axes[0, 0].hist(mus_hat, bins='auto', label=r'$\\hat{\\mu}$', alpha=0.6)\n",
    "axes[0, 0].hist(mus_exact, bins=bins, label=r'$\\mu$', alpha=0.6)\n",
    "axes[0, 0].set_title('Posterior means'.format(ntest))\n",
    "axes[0, 0].axvline(x=mus_exact.mean(), label='mean $\\hat{\\mu}$', color='C1');\n",
    "axes[0, 0].axvline(x=mus_hat.mean(), label='mean $\\mu$', color='C0');\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlabel(r'posterior mean')\n",
    "axes[0, 0].set_ylabel('counts');\n",
    "\n",
    "n, bins, p = axes[0, 1].hist(stds_hat, bins='auto', label=r'$\\hat{\\sigma}$', alpha=0.6)\n",
    "axes[0, 1].hist(stds_exact, bins=bins, label=r'$\\sigma$', alpha=0.6)\n",
    "\n",
    "axes[0, 1].set_title('Posterior stds'.format(ntest))\n",
    "\n",
    "axes[0, 1].axvline(x=stds_exact.mean(), label='mean $\\hat{\\sigma}$', color='C1');\n",
    "axes[0, 1].axvline(x=stds_hat.mean(), label='mean $\\sigma$', color='C0');\n",
    "\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel(r'posterior std')\n",
    "\n",
    "n, bins, patches = axes[0, 2].hist(dkl_ratios[:-100], bins=100, alpha=0.7,\n",
    "                                   label=r'$\\frac{D_{KL}(p(\\theta | x)||\\hat{p}(\\theta | x))}{ D_{KL}(p(\\theta | x)||p_{MLE}(\\theta | x))}$')\n",
    "# axes[0, 2].set_title(r'Histogram of $\\frac{D_{KL}(p(\\theta | x)||\\hat{p}(\\theta | x))}{ H(p(\\theta | x))}$ ')\n",
    "axes[0, 2].set_title(r'normalized $D_{KL}$')\n",
    "axes[0, 2].set_xlabel(r'$D_{KL} / H$')\n",
    "axes[0, 2].legend(fontsize=20)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "n, bins, patches = ax[0].hist(qis, bins=20, label='K-S test, p={:1.3}'.format(kst_p))\n",
    "ax[0].set_title('Posterior quantile distribution')\n",
    "ax[0].set_xlabel('quantile')\n",
    "ax[0].set_ylabel('counts')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Posterior quantile Q-Q plot')\n",
    "ax[1].plot(theo_quantiles, sample_quantiles, 'x-', label='empirical')\n",
    "ax[1].plot(theo_quantiles, theo_quantiles, label='identity line')\n",
    "ax[1].set_ylabel('empirical quantile')\n",
    "ax[1].set_xlabel(r'quantile of $U(0, 1)$')\n",
    "ax[1].legend()\n",
    "ax[1].grid();\n",
    "\n",
    "ax[2].set_title('Posterior credible intervals')\n",
    "ax[2].plot(credible_intervals, success_prob, 'x-', label='empirical')\n",
    "ax[2].plot(credible_intervals, credible_intervals, '-', label='identity line')\n",
    "ax[2].set_ylabel(r'$\\hat{P}(\\theta_o \\in CI | x_o)$')\n",
    "ax[2].set_xlabel('credible interval')\n",
    "ax[2].legend()\n",
    "ax[2].grid();\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure_folder = '../figures'\n",
    "fn = time_stamp + 'posterior_checks_1_k2_{}.png'.format(int(k2))\n",
    "fig.savefig(os.path.join(figure_folder, fn), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = time_stamp + 'posterior_checks_2_k2_{}.png'.format(int(k2))\n",
    "fig2.savefig(os.path.join(figure_folder, fn), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Quantiles for Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(qis, bins=np.linspace(0, 1, 20))\n",
    "plt.title('Histogram of posterior quantiles')\n",
    "plt.xlabel('quantile')\n",
    "plt.ylabel('counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_quantiles = np.cumsum(n / np.sum(n))\n",
    "theo_quantiles = np.cumsum(np.diff(bins))\n",
    "plt.title('Q-Q plot: empirical quantiles vs. uniform quantiles')\n",
    "plt.plot(theo_quantiles, sample_quantiles, 'o-', label='empirical quantile distribution')\n",
    "plt.plot(theo_quantiles, theo_quantiles, label='identity')\n",
    "plt.ylabel('Empirical quantiles')\n",
    "plt.xlabel(r'Theoretical quantiles of $U(0, 1)$')\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stats, kst_p) = scipy.stats.kstest(qis, cdf='uniform')\n",
    "kst_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $D_{KL}$ check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkl_ratios.sort()\n",
    "n, bins, patches = plt.hist(dkl_ratios[:-100], bins=100)\n",
    "plt.title(r'Histogram of $\\frac{D_{KL}(p(\\theta | x)||\\hat{p}(\\theta | x))}{ H(p(\\theta | x))}$ ')\n",
    "plt.xlabel(r'$D_{KL} / H$')\n",
    "plt.ylabel('counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credible intervals\n",
    "draw a $\\theta$, get $sx$ and the posterior given $sx$, get the credible interval of different percentages, say 95 to 5 in steps of 5. \n",
    "\n",
    "The interval is defined as the equal-tailed interval, meaning that there is equal amount of mass below and above the interval. That is, to get the 90% credible interval take the 5% quantile and the 95% quantile as boundaries. \n",
    "\n",
    "Then, for each interval, check whether the true parameter falls into that interval and count a success, if yes. \n",
    "\n",
    "Repeat these steps many times to estimate the probability of the true parameter to fall into a corresponding credible interval. This probability should correspond to the corresponding amount mass in the interval. Thus, plotting the estimated probabilities against the interval values should give the identity line. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# approximate interval success probability with relative frequency \n",
    "success_prob = success_counts / ntest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Credible intervals from {} test samples')\n",
    "plt.plot(credible_intervals, credible_intervals, 'o-', label='identity line')\n",
    "plt.plot(credible_intervals, success_prob, 'o-', label='results')\n",
    "plt.xlabel('Interval')\n",
    "plt.ylabel('Relative frequency')\n",
    "plt.legend();\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diffs = mus_exact - mus_hat\n",
    "std_diffs = stds_exact - stds_hat\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "n, bins, p = ax1.hist(mus_hat, bins='auto', label=r'$\\hat{\\mu}$', alpha=0.7)\n",
    "ax1.hist(mus_exact, bins=bins, label=r'$\\mu$', alpha=0.7)\n",
    "ax1.set_title('Histogram of posterior means, {} test draws'.format(ntest))\n",
    "ax1.axvline(x=mus_exact.mean(), label='mean $\\hat{\\mu}$', color='r');\n",
    "ax1.axvline(x=mus_hat.mean(), label='mean $\\mu$', color='g');\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(r'posterior mean')\n",
    "ax1.set_ylabel('counts');\n",
    "\n",
    "n, bins, p = ax2.hist(stds_hat, bins='auto', label=r'$\\hat{\\sigma}$', alpha=0.7)\n",
    "ax2.hist(stds_exact, bins=bins, label=r'$\\sigma$', alpha=0.7)\n",
    "\n",
    "ax2.set_title('Histogram of posterior stds, {} test draws'.format(ntest))\n",
    "\n",
    "ax2.axvline(x=stds_exact.mean(), label='mean $\\hat{\\sigma}$', color='r');\n",
    "ax2.axvline(x=stds_hat.mean(), label='mean $\\sigma$', color='g');\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(r'posterior std');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
