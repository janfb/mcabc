{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain model comparison toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import time \n",
    "import tqdm\n",
    "\n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import *\n",
    "from model_comparison.models import PoissonModel, NegativeBinomialModel\n",
    "\n",
    "from delfi.distribution.mixture import MoG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Poisson and Negative Binomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 2\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "time_stamp = time.strftime('%Y%m%d%H%M_')\n",
    "figure_folder = '../figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 10\n",
    "ntrain = 100000\n",
    "ntest = 100\n",
    "\n",
    "k2 = 1.\n",
    "theta2 = 1.0\n",
    "\n",
    "k3 = 2.\n",
    "theta3 = 2. \n",
    "\n",
    "# then the scale of the Gamma prior for the Poisson is given by\n",
    "theta1 = 2.0\n",
    "k1 = (k2 * theta2 * k3 * theta3) / theta1\n",
    "print(k1)\n",
    "\n",
    "\n",
    "model_poisson = PoissonModel(sample_size=sample_size, seed=seed, n_workers=2)\n",
    "model_nb = NegativeBinomialModel(sample_size=sample_size, seed=seed, n_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate parameters from the priors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Gamma prior for Poisson \n",
    "prior_lam = scipy.stats.gamma(a=k1, scale=theta1)\n",
    "prior_k = scipy.stats.gamma(a=k2, scale=theta2)\n",
    "prior_theta = scipy.stats.gamma(a=k3, scale=theta3)\n",
    "\n",
    "n = ntrain + ntest\n",
    "params_poi = prior_lam.rvs(size=int(n / 2))\n",
    "params_nb = np.vstack((prior_k.rvs(size=int(n / 2)), \n",
    "                       prior_theta.rvs(size=int(n / 2)))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data from models and calculate summary stats, prepare test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_poi = model_poisson.gen(params_poi)\n",
    "data_nb = model_nb.gen(params_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle and set up model index target vector \n",
    "x_all = np.vstack((data_poi, data_nb))\n",
    "\n",
    "# define model indices\n",
    "m_all = np.hstack((np.zeros(data_poi.shape[0]), np.ones(data_nb.shape[0]))).squeeze().astype(int)\n",
    "\n",
    "# get shuffled indices \n",
    "shuffle_indices = np.arange(n)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "# shuffle the data \n",
    "x_all = x_all[shuffle_indices, ]\n",
    "m_all = m_all[shuffle_indices].tolist()\n",
    "\n",
    "x, xtest = x_all[:ntrain, :], x_all[ntrain:, :]\n",
    "m, mtest = m_all[:ntrain], m_all[ntrain:]\n",
    "\n",
    "# calculate summary stats\n",
    "sx = calculate_stats_toy_examples(x)\n",
    "sx_test = calculate_stats_toy_examples(xtest)\n",
    "# use training norm to normalize test data \n",
    "sx_zt, training_norm = normalize(sx)\n",
    "sx_test_zt, training_norm = normalize(sx_test, training_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the NN and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ClassificationMDN(n_input=2, n_hidden_units=10, n_hidden_layers=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(model, optimizer, verbose=True, classification=True)\n",
    "\n",
    "n_epochs = 100 \n",
    "n_minibatch = int(ntrain / 100)\n",
    "\n",
    "# train with training data\n",
    "loss_trace = trainer.train(sx_zt, m, n_epochs=n_epochs, n_minibatch=n_minibatch)\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppoi_exact = []\n",
    "with tqdm.tqdm(total=len(mtest), desc='Calculate evidences for test samples', ncols=110) as pbar: \n",
    "    for xi in xtest: \n",
    "        nb_logevi = calculate_nb_evidence(xi, k2, theta2, k3, theta3, log=True)\n",
    "        poi_logevi = poisson_evidence(xi, k=k1, theta=theta1, log=True)\n",
    "        ppoi_exact.append(calculate_pprob_from_evidences(np.exp(poi_logevi), np.exp(nb_logevi)))\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms, vs = np.meshgrid(np.linspace(0, 300, 100), np.linspace(0, 400, 100))\n",
    "# stack values to evaluate as vector in the model \n",
    "sx_vis = np.vstack((ms.flatten(), vs.flatten())).T\n",
    "# normalize \n",
    "sx_vis, training_norm = normalize(sx_vis, training_norm)\n",
    "\n",
    "\n",
    "# predict probs \n",
    "ppoi_vec = model.predict(sx_vis)\n",
    "# take poisson posterior prob and reshape to grid dimensions\n",
    "ppoi_vismat = ppoi_vec[:, 0].reshape(ms.shape[0], vs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "cnorm = mpl.colors.Normalize(vmin=ppoi_vismat.min(), vmax=ppoi_vismat.max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "plt.scatter(x=sx_test[:, 0], y=sx_test[:, 1], c=np.array(ppoi_exact), cmap=cmap, \n",
    "            norm=cnorm, edgecolors='r', linewidths=0.3)\n",
    "plt.imshow(ppoi_vismat, origin='lower', aspect='auto', \n",
    "           norm=cnorm, cmap=cmap, extent=[ms.min(), ms.max(), vs.min(), vs.max()])\n",
    "\n",
    "plt.xlabel('Sample mean')\n",
    "plt.ylabel('Sample variance')\n",
    "plt.colorbar(label='P(Poisson | x)', pad=0.01)\n",
    "plt.legend(['Exact posterior probabilities'])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe Poisson data and predict underlying model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a single sample of Poisson data, with lambda at the mean of the model prior \n",
    "true_lambda = rng.gamma(shape=k1, scale=theta1)\n",
    "x_obs = rng.poisson(lam=true_lambda, size=sample_size)\n",
    "# calculate stats \n",
    "sx_obs = calculate_stats_toy_examples(x_obs)\n",
    "# normalize using training data normalization \n",
    "sx_obs_zt, training_norm = normalize(sx_obs, training_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict \n",
    "p_vec = model.predict(sx_obs_zt).squeeze()\n",
    "# calculate exact evidence \n",
    "nb_evidence = calculate_nb_evidence(x_obs, k2, theta2, k3, theta3, log=True)\n",
    "poi_evidence = poisson_evidence(x_obs, k=k1, theta=theta1, log=True)\n",
    "ppoi_ana = calculate_pprob_from_evidences(np.exp(poi_evidence), np.exp(nb_evidence))\n",
    "\n",
    "print(r'predicted P(poisson | data) = {:.2f}'.format(p_vec[0]))\n",
    "print(r'exact P(poisson | data) = {:.2f}'.format(ppoi_ana))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_modelpost = dict(seed=seed, trainer=trainer, model_models=model,\n",
    "                       x=x, m=m, xtest=xtest, mtest=mtest, \n",
    "                       sx=sx, sx_test=sx_test, \n",
    "                       training_norm=training_norm, param_poi=params_poi, params_nb=params_nb, \n",
    "                       theta1=theta1, theta2=theta2, theta3=theta3, \n",
    "                       k1=k1, k2=k2, k3=k3, \n",
    "                       sample_size=sample_size, n_samples=ntrain, \n",
    "                       ppoi_exact=ppoi_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the predicted underlying model we can learn the posterior of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG \n",
    "model_params_mdn = UnivariateMogMDN(ndim_input=2, n_hidden=10, n_components=2)\n",
    "optimizer = torch.optim.Adam(model_params_mdn.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_params_mdn, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate stats for poisson model \n",
    "sx_poi = calculate_stats_toy_examples(data_poi)\n",
    "\n",
    "# normalize data \n",
    "sx_poi_zt, data_norm = normalize(sx_poi)\n",
    "\n",
    "# normalize prior params \n",
    "params_poi_zt, prior_norm = normalize(params_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_trace = trainer.train(sx_poi_zt, params_poi_zt, n_epochs=100, n_minibatch=int(n / 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_thetas = 1000\n",
    "thetas_poisson = np.linspace(0, 20, n_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the observed data \n",
    "sx_obs_zt, data_norm = normalize(sx_obs, data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get delfi MoG for plotting and inverse z transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a single sample of Poisson data, with lambda at the mean of the model prior \n",
    "true_lambda = rng.gamma(shape=k1, scale=theta1)\n",
    "x_obs = rng.poisson(lam=true_lambda, size=sample_size)\n",
    "# calculate stats \n",
    "sx_obs = calculate_stats_toy_examples(x_obs)\n",
    "# normalize using training data normalization \n",
    "sx_obs_zt, _ = normalize(sx_obs, data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(out_mu, out_sigma, out_alpha) = model_params_mdn(Variable(torch.Tensor(sx_obs_zt)))\n",
    "\n",
    "# convert to dd format \n",
    "a = out_alpha.data.numpy().squeeze().tolist()\n",
    "ms = [[m] for m in out_mu.data.numpy().squeeze().tolist()]\n",
    "Ss = [[[s**2]] for s in out_sigma.data.numpy().squeeze().tolist()]\n",
    "\n",
    "# set up dd MoG object \n",
    "mog_posterior_delfi_zt = MoG(a=a, ms=ms, Ss=Ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform back to prior ranges \n",
    "mog_posterior_delfi = mog_posterior_delfi_zt.ztrans_inv(mean=prior_norm[0], std=prior_norm[1])\n",
    "posterior_pdvalues = mog_posterior_delfi.eval(x=thetas_poisson.reshape(1000, -1), log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get true posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get analytical gamma posterior \n",
    "k_post = k1 + np.sum(x_obs)\n",
    "\n",
    "# use the posterior given the summary stats, not the data vector \n",
    "scale_post = 1. / (sample_size + theta1**-1)\n",
    "\n",
    "# somehow we have to scale with N again, why? because the scale is changed due to s(x) by 1/N  \n",
    "true_post_poisson = gamma.pdf(x=thetas_poisson, a=k_post, scale=scale_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(r'Gamma posterior fit for Poisson $\\lambda$')\n",
    "# plt.plot(thetas_poisson, gamma_prior.pdf(thetas_poisson), label='prior')\n",
    "plt.plot(thetas_poisson, true_post_poisson, label='analytical posterior')\n",
    "plt.plot(thetas_poisson, posterior_pdvalues, label='predicted posterior')\n",
    "plt.axvline(x=k1 * theta1, color=\"C2\", label='true lambda')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\lambda$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate test data \n",
    "ntest = 1000\n",
    "params_test = prior_lam.rvs(size=ntest)\n",
    "params_test_zt, _ = normalize(params_test, prior_norm)\n",
    "\n",
    "x_test = model_poisson.gen(params_test)\n",
    "sx_test = calculate_stats_toy_examples(x_test)\n",
    "sx_test_zt, _ = normalize(sx_test, data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qis = np.zeros(ntest)\n",
    "nbreak = ntest\n",
    "qis = np.zeros(nbreak)\n",
    "# for every test sample \n",
    "for ii, (thetao_i, sxo_i) in enumerate(zip(params_test_zt, sx_test_zt)): \n",
    "    \n",
    "    # predict the posterior\n",
    "    posterior = model_params_mdn.predict(sxo_i.reshape(1, -1))\n",
    "    \n",
    "    # get quantile of theta_o\n",
    "    qis[ii] = posterior.get_quantile(thetao_i)\n",
    "    if ii==nbreak-1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.kstest(qis, cdf='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, bins, patch = plt.hist(qis, bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## We can also observe NB data, predict idx and learn the posterior\n",
    "The NB posterior is 2D, one dimension for $k$ and once for $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG \n",
    "model_params_mdn = MultivariateMogMDN(ndim_input=2, ndim_output=2, n_hidden_units=10, \n",
    "                                      n_hidden_layers=2, n_components=2)\n",
    "optimizer = torch.optim.Adam(model_params_mdn.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_params_mdn, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate stats for poisson model \n",
    "sx_nb = calculate_stats_toy_examples(data_nb)\n",
    "\n",
    "# normalize data \n",
    "sx_nb_zt, data_norm = normalize(sx_nb)\n",
    "\n",
    "# normalize prior params \n",
    "params_nb_zt, prior_norm = normalize(params_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_trace = trainer.train(sx_nb_zt, params_nb_zt, n_epochs=200, n_minibatch=int(sx_nb.shape[0] / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(loss_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe NB data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thetao = [[prior_k.rvs(), prior_theta.rvs()]]\n",
    "xo = model_nb.gen(thetao)\n",
    "sxo = calculate_stats_toy_examples(xo)\n",
    "sxo_zt, _ = normalize(sxo, data_norm)\n",
    "thetao_zt, _ = normalize(thetao, prior_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict posterior and transform to absolut range\n",
    "phat = model_params_mdn.predict(sxo_zt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(phat.mean, thetao_zt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a way to compare to the exact posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile check \n",
    "\n",
    "ToDo: \n",
    "    - multivariate quantile check DONE \n",
    "    - derive exact posterior \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate test data \n",
    "ntest = 1000\n",
    "params_test = np.vstack((prior_k.rvs(ntest), prior_theta.rvs(ntest))).T\n",
    "params_test_zt, _ = normalize(params_test, prior_norm)\n",
    "\n",
    "x_test = model_nb.gen(params_test)\n",
    "sx_test = calculate_stats_toy_examples(x_test)\n",
    "sx_test_zt, _ = normalize(sx_test, data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qis = np.zeros(ntest)\n",
    "# for every test sample \n",
    "for ii, (thetao_i, sxo_i) in enumerate(zip(params_test_zt, sx_test_zt)): \n",
    "    \n",
    "    # predict the posterior\n",
    "    posterior = model_params_mdn.predict(sxo_i.reshape(1, -1))\n",
    "    \n",
    "    # get quantile of theta_o\n",
    "    qis[ii] = posterior.get_quantile(thetao_i.reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(qis, bins='auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "either the fit is very bad or the quantile calculation is wrong. double check by deriving the exact posterior and compute it numerically. then one can see the goodness of the fit in terms of dkl, means and std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
