{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain model comparison toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import ClassificationSingleLayerMDN, Trainer, UnivariateMogMDN\n",
    "from model_comparison.models import PoissonModel, NegativeBinomialModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Poisson and Negative Binomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = None\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "sample_size = 10\n",
    "ntrain = 1000\n",
    "\n",
    "k2 = 5.\n",
    "theta2 = 2.0\n",
    "\n",
    "k3 = 2.\n",
    "theta3 = .8\n",
    "\n",
    "# then the scale of the Gamma prior for the Poisson is given by\n",
    "k1 = 2.0\n",
    "theta1 = (k2 * theta2 * k3 * theta3) / k1\n",
    "print(theta1)\n",
    "\n",
    "model_poisson = PoissonModel(k1, theta1, sample_size=sample_size, seed=seed)\n",
    "model_nb = NegativeBinomialModel(k2, theta2, k3, theta3, sample_size=sample_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_poi = rng.gamma(shape=k1, scale=theta1, size=int(ntrain / 2))\n",
    "\n",
    "params_nb = np.vstack((rng.gamma(shape=k2, scale=theta2, size=int(ntrain / 2)), \n",
    "                       rng.gamma(shape=k3, scale=theta3, size=int(ntrain / 2)))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data from models and calculate summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_poi = model_poisson.gen(int(ntrain / 2))\n",
    "# data_nb = model_nb.gen(int(ntrain / 2))\n",
    "\n",
    "data_poi = model_poisson.gen(params_poi)\n",
    "data_nb = model_nb.gen(params_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and set up model index target vector \n",
    "x = np.vstack((data_poi, data_nb))\n",
    "\n",
    "# define model indices\n",
    "m = np.hstack((np.zeros(data_poi.shape[0]), np.ones(data_nb.shape[0]))).squeeze().astype(int)\n",
    "\n",
    "# shuffle data\n",
    "shuffle_indices = np.arange(ntrain)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "x = x[shuffle_indices,]\n",
    "m = m[shuffle_indices].tolist()\n",
    "\n",
    "# calculate summary stats\n",
    "sx = calculate_stats_toy_examples(x)\n",
    "sx, training_norm = normalize(sx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the NN and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationSingleLayerMDN(ndim_input=2, n_hidden=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(model, optimizer, verbose=True, classification=True)\n",
    "\n",
    "n_epochs = 200 \n",
    "n_minibatch = 10\n",
    "\n",
    "# train with training data\n",
    "loss_trace = trainer.train(sx, m, n_epochs=n_epochs, n_minibatch=n_minibatch)\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe Poisson data and predict underlying model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = rng.poisson(lam=k1*theta1, size=sample_size)\n",
    "sx_obs = calculate_stats_toy_examples(x_obs).squeeze()\n",
    "sx_obs, training_norm = normalize(sx_obs, training_norm)\n",
    "\n",
    "softmax = nn.Softmax(dim=0)\n",
    "out_act = model(Variable(torch.Tensor(sx_obs)))\n",
    "p_vec = softmax(out_act).data.numpy()\n",
    "print('P(poisson | data) = {:.2f}'.format(p_vec[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the predicted underlying model we can learn the posterior of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG \n",
    "model = UnivariateMogMDN(ndim_input=2, n_hidden=10, n_components=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(model, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_poi = calculate_stats_toy_examples(data_poi)\n",
    "sx_poi, training_norm = normalize(sx_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_trace = trainer.train(sx_poi, params_poi, n_epochs=100, n_minibatch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_thetas = 1000\n",
    "thetas_poisson = np.linspace(0, 30, n_thetas)\n",
    "post_poisson = get_mog_posterior(model, sx_obs, thetas_poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true posteriors \n",
    "# get analytical gamma posterior \n",
    "k_post = k1 + np.sum(x_obs)\n",
    "\n",
    "# use the posterior given the summary stats, not the data vector \n",
    "scale_post = 1. / (sample_size + theta1**-1)\n",
    "\n",
    "# somehow we have to scale with N again, why? because the scale is changed due to s(x) by 1/N  \n",
    "true_post_poisson = gamma.pdf(x=thetas_poisson, a=k_post, scale=scale_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(r'Gamma posterior fit for Poisson $\\lambda$')\n",
    "# plt.plot(thetas_poisson, gamma_prior.pdf(thetas_poisson), label='prior')\n",
    "plt.plot(thetas_poisson, true_post_poisson, label='analytical posterior')\n",
    "plt.plot(thetas_poisson, post_poisson.data.numpy(), label='predicted posterior')\n",
    "plt.axvline(x=k1 * theta1, color=\"C2\", label='true lambda')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\lambda$');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
