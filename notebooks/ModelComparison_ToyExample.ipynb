{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain model comparison toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "from model_comparison.utils import *\n",
    "from model_comparison.mdns import ClassificationSingleLayerMDN, Trainer, UnivariateMogMDN\n",
    "from model_comparison.models import PoissonModel, NegativeBinomialModel\n",
    "\n",
    "from delfi.distribution.mixture import MoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Poisson and Negative Binomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "sample_size = 10\n",
    "ntrain = 100000\n",
    "\n",
    "k2 = 5.\n",
    "theta2 = 2.0\n",
    "\n",
    "k3 = 20\n",
    "theta3 = .1\n",
    "\n",
    "# then the scale of the Gamma prior for the Poisson is given by\n",
    "k1 = 2.0\n",
    "theta1 = (k2 * theta2 * k3 * theta3) / k1\n",
    "print(theta1)\n",
    "\n",
    "model_poisson = PoissonModel(sample_size=sample_size, seed=seed, n_workers=2)\n",
    "model_nb = NegativeBinomialModel(sample_size=sample_size, seed=seed, n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k1 * theta1) \n",
    "r = k2 * theta2 \n",
    "th = k2 * theta3\n",
    "p = th / (1 + th)\n",
    "print(r * 1 / ((1 - p)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate parameters from the priors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Gamma prior for Poisson \n",
    "params_poi = rng.gamma(shape=k1, scale=theta1, size=int(ntrain / 2))\n",
    "\n",
    "# from two Gamma priors for indirect Poisson-Gamma Sampling.\n",
    "params_nb = np.vstack((rng.gamma(shape=k2, scale=theta2, size=int(ntrain / 2)), \n",
    "                       rng.gamma(shape=k3, scale=theta3, size=int(ntrain / 2)))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data from models and calculate summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_poi = model_poisson.gen(params_poi)\n",
    "data_nb = model_nb.gen(params_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and set up model index target vector \n",
    "x = np.vstack((data_poi, data_nb))\n",
    "\n",
    "# define model indices\n",
    "m = np.hstack((np.zeros(data_poi.shape[0]), np.ones(data_nb.shape[0]))).squeeze().astype(int).tolist()\n",
    "\n",
    "# calculate summary stats\n",
    "sx = calculate_stats_toy_examples(x)\n",
    "sx_zt, training_norm = normalize(sx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the NN and train it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationSingleLayerMDN(ndim_input=2, n_hidden=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(model, optimizer, verbose=True, classification=True)\n",
    "\n",
    "n_epochs = 200 \n",
    "n_minibatch = int(ntrain / 100)\n",
    "\n",
    "# train with training data\n",
    "loss_trace = trainer.train(sx_zt, m, n_epochs=n_epochs, n_minibatch=n_minibatch)\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe Poisson data and predict underlying model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a single sample of Poisson data, with lambda at the mean of the model prior \n",
    "x_obs = rng.poisson(lam=k1*theta1, size=sample_size)\n",
    "# calculate stats \n",
    "sx_obs = calculate_stats_toy_examples(x_obs)\n",
    "# normalize using training data normalization \n",
    "sx_obs_zt, training_norm = normalize(sx_obs, training_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "p_vec = model(Variable(torch.Tensor(sx_obs_zt))).data.numpy().squeeze()\n",
    "print('P(poisson | data) = {:.2f}'.format(p_vec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_obs, norm): \n",
    "    sx_obs = calculate_stats_toy_examples(x_obs)\n",
    "    # normalize using training data normalization \n",
    "    sx_obs_zt, norm = normalize(sx_obs, norm)\n",
    "\n",
    "    # predict \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    out_act = model(Variable(torch.Tensor(sx_obs_zt)))\n",
    "    p_vec = softmax(out_act).data.numpy().squeeze()\n",
    "    \n",
    "    return p_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the predicted underlying model we can learn the posterior of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG \n",
    "model_params_mdn = UnivariateMogMDN(ndim_input=2, n_hidden=10, n_components=3)\n",
    "optimizer = torch.optim.Adam(model_params_mdn.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_params_mdn, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate stats for poisson model \n",
    "sx_poi = calculate_stats_toy_examples(data_poi)\n",
    "\n",
    "# normalize data \n",
    "sx_poi_zt, data_norm = normalize(sx_poi)\n",
    "\n",
    "# normalize prior params \n",
    "params_poi_zt, prior_norm = normalize(params_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_trace = trainer.train(sx_poi_zt, params_poi_zt, n_epochs=100, n_minibatch=int(ntrain / 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_thetas = 1000\n",
    "thetas_poisson = np.linspace(0, 30, n_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the observed data \n",
    "sx_obs_zt, data_norm = normalize(sx_obs, data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get delfi MoG for plotting and inverse z transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out_alpha, out_sigma, out_mu) = model_params_mdn(Variable(torch.Tensor(sx_obs_zt)))\n",
    "\n",
    "# convert to dd format \n",
    "a = out_alpha.data.numpy().squeeze().tolist()\n",
    "ms = [[m] for m in out_mu.data.numpy().squeeze().tolist()]\n",
    "Ss = [[[s**2]] for s in out_sigma.data.numpy().squeeze().tolist()]\n",
    "\n",
    "# set up dd MoG object \n",
    "mog_posterior_delfi_zt = MoG(a=a, ms=ms, Ss=Ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform back to prior ranges \n",
    "mog_posterior_delfi = mog_posterior_delfi_zt.ztrans_inv(mean=prior_norm[0], std=prior_norm[1])\n",
    "posterior_pdvalues = mog_posterior_delfi.eval(x=thetas_poisson.reshape(1000, -1), log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get true posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get analytical gamma posterior \n",
    "k_post = k1 + np.sum(x_obs)\n",
    "\n",
    "# use the posterior given the summary stats, not the data vector \n",
    "scale_post = 1. / (sample_size + theta1**-1)\n",
    "\n",
    "# somehow we have to scale with N again, why? because the scale is changed due to s(x) by 1/N  \n",
    "true_post_poisson = gamma.pdf(x=thetas_poisson, a=k_post, scale=scale_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(r'Gamma posterior fit for Poisson $\\lambda$')\n",
    "# plt.plot(thetas_poisson, gamma_prior.pdf(thetas_poisson), label='prior')\n",
    "plt.plot(thetas_poisson, true_post_poisson, label='analytical posterior')\n",
    "plt.plot(thetas_poisson, posterior_pdvalues, label='predicted posterior')\n",
    "plt.axvline(x=k1 * theta1, color=\"C2\", label='true lambda')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\lambda$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss trace \n",
    "\n",
    "several mdns with different data\n",
    "\n",
    "take mean over several tries\n",
    "\n",
    "use direct regression without tanh layer\n",
    "\n",
    "difficulty: \n",
    "difference in variance of samples\n",
    "\n",
    "just change xobs: small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
