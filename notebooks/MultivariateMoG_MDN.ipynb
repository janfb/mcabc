{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from scipy.stats import beta\n",
    "import scipy.special\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=10, n_components=1):\n",
    "        super(MDN, self).__init__()\n",
    "        \n",
    "        self.ndims = ndim_output \n",
    "        self.n_components = n_components\n",
    "        # the number of entries in the upper triangular Choleski transform matrix of the precision matrix \n",
    "        self.utriu_entries = int(self.ndims * (self.ndims - 1) / 2) + self.ndims\n",
    "\n",
    "        # input layer \n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        # activation \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # output layer the mean estimates \n",
    "        self.mu_out = nn.Linear(n_hidden, ndim_output * n_components)\n",
    "\n",
    "        # output layer to precision estimates \n",
    "        # the upper triangular matrix for D-dim Gaussian has m = (D**2 + D) / 2 entries \n",
    "        # this should be a m-vector for every component. currently it is just a scalar for every component. \n",
    "        # or it could be a long vector of length m * k, i.e, all the k vector stacked. \n",
    "        self.U_out = nn.Linear(n_hidden, self.utriu_entries * n_components)\n",
    "        \n",
    "        # additionally we have the mixture weights alpha \n",
    "        self.alpha_out = nn.Linear(n_hidden, n_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        \n",
    "        out_mu = self.mu_out(act)\n",
    "        out_mu = out_mu.view(batch_size, self.ndims, self.n_components)\n",
    "        out_alpha = self.softmax(self.alpha_out(act))\n",
    "\n",
    "        # get activate of upper triangle U vector\n",
    "        U_vec = self.U_out(act)\n",
    "        # prelocate U matrix \n",
    "        U_mat = Variable(torch.zeros(batch_size, self.n_components, self.ndims, self.ndims))\n",
    "        \n",
    "        # assign vector to upper triangle of U \n",
    "        (idx1, idx2) = np.triu_indices(self.ndims)\n",
    "        U_mat[:, :, idx1, idx2] = U_vec\n",
    "        # apply exponential to get positive diagonal\n",
    "        (idx1, idx2) = np.diag_indices(self.ndims)\n",
    "        U_mat[:, :, idx1, idx2] = torch.exp(U_mat[:, :, idx1, idx2])\n",
    "        \n",
    "        return (out_mu, U_mat, out_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multivariateMoG_loss(y, mu, U, alpha):\n",
    "    \n",
    "    result = multivariateMoG_pdf(y, mu, U, alpha, log=True)    \n",
    "    \n",
    "    result = torch.mean(result)  # mean over batch\n",
    "    return -result\n",
    "\n",
    "def multivariateMoG_pdf(y, mus, Us, alphas, log=False): \n",
    "    \n",
    "    # get params: batch size N, ndims D, ncomponents K\n",
    "    N, D, K = mus.size()\n",
    "        \n",
    "    # prelocate matrix for log probs of each Gaussian component\n",
    "    log_probs_mat = Variable(torch.zeros(N, K))\n",
    "    \n",
    "    # take weighted sum over components to get log probs \n",
    "    for k in range(K): \n",
    "        log_probs_mat[:, k] = multivariate_normal_pdf(X=y, mus=mus[:, :, k], Us=Us[:, k, :, :], log=True)\n",
    "    \n",
    "    # now apply the log sum exp trick: sum_k alpha_k * N(Y|mu, sigma) = sum_k exp(log(alpha_k) + log(N(Y| mu, sigma)))\n",
    "    # this give the log MoG density over the batch\n",
    "    log_probs_batch = my_log_sum_exp(torch.log(alphas) + log_probs_mat, axis=1)  # sum over component axis=1\n",
    "    \n",
    "    # return log or linear density dependent on flag: \n",
    "    if log: \n",
    "        result = log_probs_batch\n",
    "    else: \n",
    "        result = torch.exp(log_probs_batch)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_2DGaussian_dataset(n_samples, sample_size, prior, data_cov=None):\n",
    "\n",
    "    X = []\n",
    "    thetas = []\n",
    "    \n",
    "    if data_cov is None: \n",
    "        data_cov = .5 * np.eye(2)\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        # sample from the prior \n",
    "        theta = prior.rvs()\n",
    "\n",
    "        # generate samples with mean from prior and unit variance \n",
    "        x = scipy.stats.multivariate_normal.rvs(mean=theta, cov=data_cov, size=sample_size).reshape(sample_size, 2)\n",
    "        \n",
    "        sx = stats_ND_Gaussian(x)\n",
    "\n",
    "        # as data we append the summary stats\n",
    "        X.append(sx) \n",
    "        thetas.append([theta])   \n",
    "    \n",
    "    return np.array(X).squeeze(), np.array(thetas).squeeze()\n",
    "\n",
    "def stats_ND_Gaussian(x): \n",
    "    \"\"\"\n",
    "    Calculate the sufficient statistics of a multivariate Gaussian sample x\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(x, axis=0).astype(float)])\n",
    "        \n",
    "def train(X, Y, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))\n",
    "            y_var = Variable(torch.Tensor(y_batch))\n",
    "                        \n",
    "            (out_mu, out_U, out_alpha) = model(x_var)\n",
    "            loss = multivariateMoG_loss(y_var, out_mu, out_U, out_alpha)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data.numpy())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MDN(ndim_input=2, n_components=3)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "sample_size = 10\n",
    "    \n",
    "# prior on the mean\n",
    "data_cov = 0.5 * np.eye(2)\n",
    "prior = scipy.stats.multivariate_normal(mean=[0., 0.], cov=2. * np.eye(2))\n",
    "\n",
    "X, Y = generate_2DGaussian_dataset(n_samples, sample_size, prior, data_cov)\n",
    "X, norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior.mean.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = train(X, Y, n_epochs=50, n_minibatch=100)\n",
    "plt.plot(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# observe data \n",
    "true_mu = [-.5, .1]\n",
    "true_cov = data_cov\n",
    "xo = scipy.stats.multivariate_normal.rvs(mean=true_mu, cov=true_cov, size=sample_size).reshape(sample_size, 2)\n",
    "# gets stats \n",
    "statso = stats_ND_Gaussian(xo)\n",
    "# normalize \n",
    "statso, norm = normalize(statso, norm)\n",
    "# cast to torch \n",
    "statso = Variable(torch.Tensor(statso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(211)\n",
    "plt.hist2d(X[:, 0], X[:, 1])\n",
    "plt.title('Samples generated by the model: 2D Gaussian with prior on $\\mu$');\n",
    "plt.colorbar();\n",
    "plt.subplot(212)\n",
    "plt.title('observed data')\n",
    "plt.hist2d(xo[:, 0], xo[:, 1])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with observed data \n",
    "(out_mu, out_U, out_alpha) = model(statso.view(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_mean, post_cov = calculate_multivariate_normal_mu_posterior(xo, data_cov, sample_size, \n",
    "                                                                 prior.mean, prior.cov)\n",
    "postana = scipy.stats.multivariate_normal(mean=post_mean, cov=post_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = 1.\n",
    "x, y = np.mgrid[-r:r:.01, -r:r:.01]\n",
    "pos = np.dstack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos.shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the grid of PDF values for the MoG: \n",
    "# extract torch variables for every component and take weighted sum with alpha weights\n",
    "posterior_grid = np.zeros((200, 200))\n",
    "\n",
    "# for every component \n",
    "for k in range(3): \n",
    "    alpha = out_alpha[0, k].data.numpy()\n",
    "    mean = out_mu[0, :, k].data.numpy()\n",
    "    U = out_U[0, k, ].data.numpy()\n",
    "    # get cov from Choleski transform \n",
    "    cov = np.linalg.inv(U.T.dot(U))\n",
    "    # add to result, weighted with alpha \n",
    "    posterior_grid += alpha * scipy.stats.multivariate_normal.pdf(x=pos, mean=mean, cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "plt.subplot(311)\n",
    "plt.contourf(x, y, prior.pdf(pos))\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro')\n",
    "plt.title('Prior')\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.contourf(x, y, posterior_grid)\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro')\n",
    "plt.title('Posterior')\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.contourf(x, y, postana.pdf(pos))\n",
    "plt.title('Analytical Posterior')\n",
    "plt.plot(true_mu[0], true_mu[1], 'ro');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
