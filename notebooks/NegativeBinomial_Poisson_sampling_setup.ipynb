{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl_params = {'legend.fontsize': 15,\n",
    "                      'axes.titlesize': 20,\n",
    "                      'axes.labelsize': 17,\n",
    "                      'xtick.labelsize': 12,\n",
    "                      'ytick.labelsize': 12,\n",
    "             'figure.figsize' : (18, 5)}\n",
    "\n",
    "mpl.rcParams.update(mpl_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Negative Binomial (NB) distribution\n",
    "\n",
    "To better control and compare the prior parameters of the NB we sample from it indirectly by sampling from a Poisson distribution with parameter $\\lambda$ which we sample from a Gamma distribution for which we define prior parameters $k$ and $\\theta$. On these prior parameters we again choose Gamma priors with fixed parameters. \n",
    "\n",
    "Normally, a negative binomial is defined as the probability distribution describing the number of successes $k$ in a sequence of independent Bernoulli trials before a predefined number of $r$ failures happen, or vice versa, the number of failures $k$ before a predefined number of successes $r$, given a probability of success / failure $p$.  \n",
    "\n",
    "\\begin{align}\n",
    "p(k | r, p) &= {k + r - 1 \\choose k} (1 - p)^r p^k\n",
    "\\end{align}\n",
    "\n",
    "However, it can also be described as a mixture of a Poisson and a Gamma distribution, such that the $\\lambda$ parameter of the mean of the Poisson is itself a RV following a Gamma distribution with shape parameter $k$ and scale parameter $\\theta$. If we choose $k=r$ and $\\theta = p/(1-p)$ then we can show that the expectation over $\\lambda$ follows a negative binomial distribution: \n",
    "\n",
    "\\begin{align}\n",
    "f(k, r, p) &= \\int_0^{\\infty} f_{Poisson(\\lambda)}(k) \\;\\; f_{Gamma(r, (1-p)/p)}(\\lambda) d\\lambda \\\\\n",
    "            &= \\int_0^{\\infty} \\frac{\\lambda^k}{k!} e^{-\\lambda} \\lambda^{r - 1} \\frac{e^{-\\lambda (1-p)/p}}{\\left(\\frac{1-p}{p}\\right )^r \\Gamma(r)} d\\lambda \\\\\n",
    "            &= \\frac{(1-p)^r p^{-r}}{k! \\Gamma(r)} \\int_0^{\\infty} \\lambda^{r + k - 1} e^{-\\lambda / p} d\\lambda \\\\\n",
    "            &= \\frac{(1-p)^r p^{-r}}{k! \\Gamma(r)} p^{r+k} \\Gamma(r + k) \\\\\n",
    "            &= \\frac{\\Gamma(k + r)}{k!\\Gamma(r)} (1-p)^r p^k \\\\\n",
    "            &= {k + r - 1 \\choose k}  (1-p)^r p^k\n",
    "\\end{align}\n",
    "\n",
    "Thus, instead of sampling from the NB directly specifying $r$ and $p$, we instead sample from a Poisson with $\\lambda$ sampled from a Gamma with shape $r$ and scale $p / (1-p)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "\n",
    "# first sample from a NB directly\n",
    "n_success = 5\n",
    "p_success = 0.5\n",
    "n_samples = 1000\n",
    "nb = scipy.stats.nbinom(n_success, p_success)\n",
    "samples_directly = nb.rvs(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can in principle generate the same set of samples by sampling from a Poisson with lambda coming from Gamma: \n",
    "def sample_nb(shape, scale, size): \n",
    "    # sample a lambda from a Gamma \n",
    "    lamb = scipy.stats.gamma.rvs(a=shape, scale=scale, size=size)\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    for l in lamb: \n",
    "        samples.append(scipy.stats.poisson.rvs(mu=l, size=1))\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n",
    "# we just have to define the shape and scale of the Gamma accordingly\n",
    "shape = n_success\n",
    "scale = p_success / (1 - p_success)\n",
    "samples_mixture = sample_nb(shape, scale, size=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.hist(samples_directly, bins='auto', color='C0')\n",
    "plt.ylim([0, 150])\n",
    "plt.title('Sampled directly from a NB')\n",
    "plt.subplot(122)\n",
    "plt.ylim([0, 150])\n",
    "plt.hist(samples_mixture, bins='auto', color='C1');\n",
    "plt.title(r'Sampled from a Poisson($\\lambda$) with $\\lambda \\sim Gamma$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sample from a Poisson and NB with equal mean but overdispersion in the NB\n",
    "\n",
    "For the model comparison toy example we want to generate data according to two models, a Poisson model that assumes mean and variance in the data to be equal and a Negative Binomial model that assumes overdispersion. We want our method to do the model comparison based on the overdispersion in the data and not based on differences in the means of the data generated by the models. For both models we define priors on their parameters that influence the means and variances of data sets generated from them, thus, those means and variances will vary from sample to sample. However, we want the mean to be the same on average. So, the expectation of the means under the priors should be the same for both the Poisson and the NB model. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{E}[\\mu_{NB}] &= \\mathbf{E}[\\mu_{Poi}] \\\\\n",
    "\\mathbf{E}[r \\frac{p}{1-p}] &= \\mathbf{E}[\\lambda]\n",
    "\\end{align}\n",
    "If we model the NB indirectly using Poisson-Gamma mixing with Gamma priors on the shape $k$ and scale $\\theta$ and use a the conjugate Gamma prior on $\\lambda$ for the Poisson model then we have \n",
    "\\begin{align}\n",
    "\\lambda &\\sim Gamma(k_1, \\theta_1) \\\\\n",
    "r = k &\\sim Gamma(k_2, \\theta_2) \\\\\n",
    "\\frac{p}{1-p} = \\theta &\\sim Gamma(k_3, \\theta_3)\n",
    "\\end{align}\n",
    "Furthermore, because $k$ and $theta$ are independent we have \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{E}[r \\frac{p}{1-p}] &= \\mathbf{E}[\\lambda] \\\\ \n",
    "\\mathbf{E}[r] \\mathbf{E}[\\theta] &= k_1 \\theta_1 \\\\ \n",
    "k_2 \\theta_2 k_3 \\theta_3 &= k_1 \\theta_1 \\\\ \n",
    "\\end{align}\n",
    "\n",
    "Given that we will choose all the hyper prior parameter by hand, we can make sure that this condition is satisfied in order to have equal expectations of the means of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the shape or scale of the Gamma prior for the Poisson model\n",
    "theta1 = 2.0 \n",
    "# set the shape and scale of the prior on the shape of the Gamma for the mixture to be broad \n",
    "theta2 = 2.0\n",
    "k2 = 5.\n",
    "# set the shape and scale of the prior on the scale of the Gamma for the mixture to be small \n",
    "# this will make the variance and could be the tuning point of the amount of overdispersion / difficulty\n",
    "theta3 = 1.0 \n",
    "k3 = 1\n",
    "\n",
    "# then the scale of the Gamma prior for the Poisson is given by \n",
    "k1 = (k2 * theta2 * k3 * theta3) / theta1\n",
    "print(k1, theta1)\n",
    "\n",
    "# get analytical means \n",
    "mean_ana_poi = k1 * theta1\n",
    "mean_ana_nb = k2 * k3 * theta2 * theta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the priors \n",
    "prior1 = scipy.stats.gamma(a=k1, scale=theta1)\n",
    "prior2 = scipy.stats.gamma(a=k2, scale=theta2)\n",
    "prior3 = scipy.stats.gamma(a=k3, scale=theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data \n",
    "n_samples = 1000\n",
    "sample_size = 10\n",
    "params_poi, samples_poi = sample_poisson(prior1, n_samples, sample_size)\n",
    "params_nb, samples_nb = sample_poisson_gamma_mixture(prior2, prior3, n_samples, sample_size)\n",
    "poi_means, poi_var = samples_poi.mean(axis=1), samples_poi.var(axis=1)\n",
    "nb_means, nb_var = samples_nb.mean(axis=1), samples_nb.var(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplot(221)\n",
    "plt.hist(poi_means, bins='auto', color='C0')\n",
    "plt.title('Poisson means, mean={:.2f}'.format(poi_means.mean()))\n",
    "plt.axvline(x=mean_ana_poi, label='analytical mean', color='C2')\n",
    "plt.axvline(x=poi_means.mean(), label='sample mean', color='C3')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('NB means, mean={:.2f}'.format(nb_means.mean()))\n",
    "plt.hist(nb_means, bins='auto', color='C1'); \n",
    "plt.axvline(x=mean_ana_nb, label='analytical mean', color='C2')\n",
    "plt.axvline(x=nb_means.mean(), label='sample mean', color='C3')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Poisson variances, mean={:.2f}'.format(poi_var.mean()))\n",
    "plt.hist(poi_var, bins='auto', color='C0'); \n",
    "plt.axvline(x=poi_var.mean(), label='sample mean', color='C3')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('NB variances, mean={:.2f}'.format(nb_var.mean()))\n",
    "plt.hist(nb_var, bins='auto', color='C1'); \n",
    "plt.axvline(x=nb_var.mean(), label='sample mean', color='C3')\n",
    "plt.legend();          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(poi_means, poi_var, 'o', color='C0', alpha=.5, label='Poisson')\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('Mean')\n",
    "plt.loglog(nb_means, nb_var, 'o', color='C1', alpha=.5, label='Neg Bin')\n",
    "plt.loglog(np.logspace(-1, 3, 1000), np.logspace(-1, 3, 1000), label='Mean = Variance')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare likelihoods of direct and indirect sampling methods\n",
    "\n",
    "For the evaluation of the marginal likelihood integral it might be necessary to evaluate the direct likelihood instead of the indirect one. The problem with evuluating the indirect one is that one needs to corresponding values of the $\\lambda$ parameter sampled from the mixture Gamma. However, when evaluating observed data these are not available. \n",
    "\n",
    "**Question**: can we evaluate the marginal likelihood integrant of the direct method even when we used the indirect sampling method to generate the data? \n",
    "\n",
    "**Answer**: Yes, if we integrate over $\\lambda$ as well. Looking at the derivation of the Poisson-Gamma mixture above we see that it is equivalent to the NB iff integrated over the whole domain of $\\lambda$. Thus, \n",
    "\n",
    "\\begin{align}\n",
    "\\int_r \\int_p p_{NB}(X | r, p)p(r, p)drdp &= \\int_r \\int_p \\int_{\\lambda} p_{poi}(X|\\lambda)p_{Gamma}(\\lambda | k, \\theta) p(k, \\theta) dk d\\theta d\\lambda \\\\ \n",
    "r = k &; \\;\\; p = \\frac{\\theta}{1 + \\theta} \\\\\n",
    "p(k, \\theta) &= p_{Gamma}(k |k_k, \\theta_k) p_{Gamma}(\\theta |k_{\\theta}, \\theta_{\\theta}) \\\\ \n",
    "\\text{To find the prior pdfs for $r$ and $p$ we need a } &\\text{change of variables from $\\theta$ to $p$ (???) then} \\\\ \n",
    "p(r, p) &= p_{Gamma}(r | k_k, \\theta_k) \\cdot \\frac{1}{(1 - p)^2}p_{Gamma}(\\theta |k_{\\theta}, \\theta_{\\theta})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check whether we can sample with $\\theta$ fixed by $r$ and $\\lambda$\n",
    "\n",
    "We want the means of the samples from Poisson and NB to match and only the variances to differ. Thus, for every sample we want \n",
    "$$\n",
    "r \\cdot \\frac{p}{1 - p} = r \\theta = \\lambda.\n",
    "$$\n",
    "We could just set $\\theta = \\lambda / r$ or since $p=\\frac{\\theta}{1+\\theta}$, $p=\\frac{\\lambda}{r + 1}$. Then we need one $\\lambda$ for every NB sample, but the means would match exactly. The variance of the NB would then be\n",
    "$$\n",
    "\\sigma^2 = r \\frac{p}{(1-p)^2}\\;\\; \\text{with} \\;\\; p=\\frac{\\lambda}{r + 1}\n",
    "$$\n",
    "\n",
    "For this scenario we actually do not need a prior on $\\theta$. Do then still have the DOG in the NB? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sample_size = 10\n",
    "params_nb, samples_nb = sample_poisson_gamma_mixture_matched_means(prior2, params_poi, n_samples, sample_size)\n",
    "nb_means, nb_var = samples_nb.mean(axis=1), samples_nb.var(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.subplot(221)\n",
    "plt.hist(poi_means, bins='auto', color='C0')\n",
    "plt.title('Poisson means, mean={:.2f}'.format(poi_means.mean()))\n",
    "plt.axvline(x=mean_ana_poi, label='analytical mean', color='C2')\n",
    "plt.axvline(x=poi_means.mean(), label='sample mean', color='C3')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('NB means, mean={:.2f}'.format(nb_means.mean()))\n",
    "plt.hist(nb_means, bins='auto', color='C1'); \n",
    "plt.axvline(x=mean_ana_nb, label='analytical mean', color='C2')\n",
    "plt.axvline(x=nb_means.mean(), label='sample mean', color='C3')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Poisson variances, mean={:.2f}'.format(poi_var.mean()))\n",
    "plt.hist(poi_var, bins='auto', color='C0'); \n",
    "plt.axvline(x=poi_var.mean(), label='sample mean', color='C3')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('NB variances, mean={:.2f}'.format(nb_var.mean()))\n",
    "plt.hist(nb_var, bins='auto', color='C1'); \n",
    "plt.axvline(x=nb_var.mean(), label='sample mean', color='C3')\n",
    "plt.legend();          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
