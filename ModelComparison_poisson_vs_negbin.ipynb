{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from scipy.stats import gamma\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Poisson data, compare Poisson vs. negBin model\n",
    "\n",
    "I will generate observed data from a Poisson distribution with a certain lambda. Then I will train a MDN to approximate the posterior over model indices. As models I use a Poisson with Gamma prior and a negative Binomial with Beta prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "true_lam = 3.\n",
    "\n",
    "# set prior parameters \n",
    "shape = 9. \n",
    "scale = .5\n",
    "alpha = 2. \n",
    "beta = 5.\n",
    "\n",
    "X_o = np.random.poisson(lam=true_lam, size=N)\n",
    "\n",
    "X_var = Variable(torch.Tensor(X_o.astype(float)))\n",
    "\n",
    "plt.hist(X_o, bins=5);\n",
    "plt.xlabel('X');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for generating data from the two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_poisson(N): \n",
    "    \n",
    "    # sample from prior\n",
    "    theta = np.random.gamma(shape, scale)\n",
    "    # generate samples\n",
    "    x = np.random.poisson(lam=theta, size=N)\n",
    "    \n",
    "    sx = np.mean(x).astype(float)\n",
    "    \n",
    "    return theta, sx\n",
    "    \n",
    "def generate_negbin(N, r=3):\n",
    "    # sample from prior\n",
    "    theta = np.random.beta(alpha, beta)\n",
    "    \n",
    "    # generate samples\n",
    "    x = np.random.negative_binomial(r, theta)\n",
    "    \n",
    "    # calculate summary stats\n",
    "    sx = np.mean(x).astype(float)\n",
    "    \n",
    "    return theta, sx\n",
    "\n",
    "def generate_dataset(n_samples, sample_size): \n",
    "    \n",
    "    X = []\n",
    "    thetas = []\n",
    "    m = []\n",
    "\n",
    "    # for every sample we want a triplet (m_i, theta, sx)\n",
    "    for i in range(n_samples): \n",
    "        \n",
    "        # sample model index \n",
    "        m_i = np.round(np.random.rand()).astype(int)\n",
    "    \n",
    "        # generate data from model \n",
    "        theta, sx = generate_negbin(N=sample_size) if m_i else generate_poisson(N=sample_size)\n",
    "    \n",
    "        X.append([sx])\n",
    "        thetas.append([theta])\n",
    "        m.append([m_i])\n",
    "    \n",
    "    return np.array(m), np.array(thetas), np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define separate networks for fitting the prior parameter posterior and the model posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(dataset, batch_size=5):\n",
    "    shuffle(dataset)\n",
    "    N_full_batches = len(dataset) // batch_size\n",
    "    for i in range(N_full_batches):\n",
    "        idx_from = batch_size * i\n",
    "        idx_to = batch_size * (i + 1)\n",
    "        xs, ys = zip(*[(x, y) for x, y in dataset[idx_from:idx_to]])\n",
    "        yield xs, ys\n",
    "\n",
    "class MDN_phi(nn.Module):\n",
    "    def __init__(self, ndim_input=1, ndim_output=1, n_hidden=5, n_components=1):\n",
    "        super(MDN_phi, self).__init__()\n",
    "        self.fc_in = nn.Linear(1, n_hidden)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.alpha_out = torch.nn.Sequential(\n",
    "              nn.Linear(n_hidden, n_components),\n",
    "              nn.Softmax())\n",
    "        self.logsigma_out = nn.Linear(n_hidden, n_components)\n",
    "        self.mu_out = nn.Linear(n_hidden, n_components)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        out_alpha = self.alpha_out(act)\n",
    "        out_sigma = torch.exp(self.logsigma_out(act))\n",
    "        out_mu = self.mu_out(act)\n",
    "        return (out_alpha, out_sigma, out_mu)\n",
    "    \n",
    "    \n",
    "class MDN_psi(nn.Module):\n",
    "    \n",
    "    def __init__(self, ndim_input=2, ndim_output=2, n_hidden=5, n_components=1):\n",
    "        super(MDN_psi, self).__init__()\n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.m_out = torch.nn.Sequential(\n",
    "              nn.Linear(n_hidden, ndim_output),\n",
    "              nn.Softmax())\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        out = self.fc_in(x, m)\n",
    "        act = self.tanh(out)\n",
    "        out_m = self.m_out(act)\n",
    "        return out_m\n",
    "    \n",
    "def train(X, Y, model, optim, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))\n",
    "            y_var = Variable(torch.Tensor(y_batch))\n",
    "                                                            \n",
    "            (out_alpha, out_sigma, out_mu) = model(x_var)\n",
    "            loss = mdn_loss_function(out_alpha, out_sigma, out_mu, y_var)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "    \n",
    "    return model, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define posterior model and corresponding loss function for learning theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this one is used to approximate the posterior with a mixture of Gaussians \n",
    "def gauss_pdf(y, mu, sigma, log=False):\n",
    "    result = -0.5*torch.log(2*np.pi*sigma**2) - 1/(2*sigma**2) * (y.expand_as(mu) - mu)**2\n",
    "    if log:\n",
    "        return result\n",
    "    else: \n",
    "        return torch.exp(result)\n",
    "\n",
    "# the loss evaluates model (MoG) with the given data (y) and takes the log loss\n",
    "def mdn_loss_function(out_alpha, out_sigma, out_mu, y):\n",
    "    result = (gauss_pdf(y, out_mu, out_sigma, log=True) * out_alpha).squeeze() # sum? \n",
    "    result = torch.mean(result)  # mean over batch\n",
    "    return -result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define something similar for learning the model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def psi_loss_function(m, out): \n",
    "    M = m.size\n",
    "    J = Variable(torch.Tensor([0]))\n",
    "\n",
    "    for i in range(M):\n",
    "        J += torch.log(out[i, m[i]])\n",
    "        \n",
    "    result = J / M\n",
    "    \n",
    "    return - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "N = 2\n",
    "m = np.random.randint(0, k, N)\n",
    "v = np.random.rand(N, k)\n",
    "print(v)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a large data set of triplets (m, theta, sx)\n",
    "\n",
    "Then separate it into sets for model 1 and model 2 and train the phi networks separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate data \n",
    "m, theta, X = generate_dataset(1000, 100)\n",
    "# separate it \n",
    "mask = m.squeeze()\n",
    "theta1, X1, = theta[mask==0], X[mask==0, :]\n",
    "theta2, X2, = theta[mask==1], X[mask==1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the phi networks for estimating the theta posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train the networks for the two models \n",
    "model1 = MDN_phi()\n",
    "optim1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "model1, optim1 = train(X1, theta1, model=model1, optim=optim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MDN_phi()\n",
    "optim2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "model2, optim2 = train(X1, theta2, model=model2, optim=optim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now evaluate the model at the observed data \n",
    "stats_o = np.array(np.mean(X_o).astype(float)).reshape(1, 1)\n",
    "\n",
    "X_var = Variable(torch.Tensor(stats_o))\n",
    "\n",
    "(out_alpha, out_sigma, out_mu) = model1(X_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posterior(model, X_o, thetas): \n",
    "    stats_o = np.array(np.mean(X_o).astype(float)).reshape(1, 1)\n",
    "\n",
    "    X_var = Variable(torch.Tensor(stats_o))\n",
    "\n",
    "    (out_alpha, out_sigma, out_mu) = model(X_var)    \n",
    "    post = []\n",
    "\n",
    "    for y in thetas: \n",
    "        y = Variable(torch.Tensor(np.array(y).reshape(1, 1)))\n",
    "        th = (gauss_pdf(y, out_mu, out_sigma, log=False) * out_alpha).sum()\n",
    "        post.append(th.data.numpy())\n",
    "    post = np.array(post).squeeze()\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thetas = np.linspace(0, 4, 100)\n",
    "post1 = get_posterior(model1, X_o, thetas)\n",
    "post2 = get_posterior(model2, X_o, thetas)\n",
    "\n",
    "prior1 = gamma.pdf(x=thetas, a=shape, loc=0, scale=scale)\n",
    "prior2 = scipy.stats.beta.pdf(x=thetas, a=alpha, b=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(thetas, post1.squeeze(), label='poisson posterior')\n",
    "plt.plot(thetas, prior1, '--', label='gamma prior')\n",
    "\n",
    "plt.plot(thetas, post2.squeeze(), label='negbin posterior')\n",
    "plt.plot(thetas, prior2, '--', label='beta prior')\n",
    "\n",
    "plt.axvline(x=true_lam, label='true theta', linestyle='--', color='k')\n",
    "plt.xlabel('theta')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
