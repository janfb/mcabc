{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from scipy.stats import beta\n",
    "import scipy.special\n",
    "from utils import *\n",
    "\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.labelsize'] = 15\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a MDN for approximating a MoG\n",
    "\n",
    "It takes as input the data $x$ **and** the model index $m$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, ndim_input=2, ndim_output=1, n_hidden=5, n_components=3):\n",
    "        super(MDN, self).__init__()\n",
    "        self.fc_in = nn.Linear(ndim_input, n_hidden)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.alpha_out = torch.nn.Sequential(\n",
    "              nn.Linear(n_hidden, n_components),\n",
    "              nn.Softmax()\n",
    "            )\n",
    "        self.logsigma_out = nn.Linear(n_hidden, n_components)\n",
    "        self.mu_out = nn.Linear(n_hidden, n_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_in(x)\n",
    "        act = self.tanh(out)\n",
    "        out_alpha = self.alpha_out(act)\n",
    "        out_sigma = torch.exp(self.logsigma_out(act))\n",
    "        out_mu = self.mu_out(act)\n",
    "        return (out_alpha, out_sigma, out_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mdn_loss_function(out_alpha, out_sigma, out_mu, y):\n",
    "\n",
    "    result = mog_pdf_new(y, mus=out_mu, sigmas=out_sigma, alphas=out_alpha, log=True)\n",
    "    result = torch.mean(result)  # mean over batch\n",
    "    return -result\n",
    "\n",
    "def train(X, Y, n_epochs=500, n_minibatch=50):\n",
    "    dataset_train = [(x, y) for x, y in zip(X, Y)]\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        bgen = batch_generator(dataset_train, n_minibatch)\n",
    "\n",
    "        for j, (x_batch, y_batch) in enumerate(bgen):\n",
    "            x_var = Variable(torch.Tensor(x_batch))\n",
    "            y_var = Variable(torch.Tensor(y_batch))\n",
    "                                                            \n",
    "            (out_alpha, out_sigma, out_mu) = model(x_var)\n",
    "            loss = mdn_loss_function(out_alpha, out_sigma, out_mu, y_var)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            losses.append(loss.data[0])\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(\"[epoch %04d] loss: %.4f\" % (epoch + 1, loss.data[0]))\n",
    "\n",
    "    return losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset_poisson(n_samples, samples_size, prior):\n",
    "    # N data sets \n",
    "    # each with m samples \n",
    "    \n",
    "    X = []\n",
    "    thetas = []\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        # sample from the prior \n",
    "        theta = prior.rvs()\n",
    "\n",
    "        # generate samples\n",
    "        x = scipy.stats.poisson.rvs(mu=theta, size=samples_size)\n",
    "        \n",
    "        # as data we append the summary stats\n",
    "        X.append(calculate_stats(x).astype(float)) \n",
    "        thetas.append([theta])\n",
    "    \n",
    "    return np.array(X), np.array(thetas)\n",
    "\n",
    "def generate_dataset_NB(n_samples, sample_size, prior):\n",
    "    # N data sets \n",
    "    # each with m samples \n",
    "    \n",
    "    X = []\n",
    "    thetas = []\n",
    "    \n",
    "    for i in range(n_samples): \n",
    "        theta = prior.rvs()\n",
    "        \n",
    "        x = nbinom.rvs(r, theta, size=sample_size)\n",
    "        \n",
    "        # as data we append the summary stats\n",
    "        X.append(calculate_stats(x).astype(float)) \n",
    "        thetas.append([theta])\n",
    "    \n",
    "    return np.array(X), np.array(thetas)\n",
    "\n",
    "import scipy.special as sps \n",
    "\n",
    "def gamma_pdf(x, k, theta): \n",
    "    return x**(k-1) * np.exp(-x / theta) / (sps.gamma(k) * theta**k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create large data set of Poisson and NB samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "sample_size = 100\n",
    "\n",
    "Ntrain = 2 * n_samples\n",
    "\n",
    "# set priors as frozen scipy distribution objects\n",
    "shape = 7.5\n",
    "scale = 1.0\n",
    "gamma_prior = scipy.stats.gamma(a=shape, scale=scale)\n",
    "alp = 5. \n",
    "bet = 2.\n",
    "r = 5.\n",
    "beta_prior = scipy.stats.beta(a=alp, b=bet)\n",
    "\n",
    "# get vector of model indices \n",
    "m_i = (np.sign(np.random.rand(Ntrain) - 0.5)).astype(int)\n",
    "\n",
    "X_poisson, Y_poisson = generate_dataset_poisson((m_i==-1).sum(), sample_size, gamma_prior)\n",
    "X_nb, Y_nb = generate_dataset_NB((m_i==1).sum(), sample_size, beta_prior)\n",
    "\n",
    "\n",
    "X = np.empty((Ntrain, 2))\n",
    "Y = np.empty((Ntrain, 1))\n",
    "X[m_i==-1, 0] = X_poisson.squeeze()\n",
    "Y[m_i==-1, 0] = Y_poisson.squeeze()\n",
    "\n",
    "X[m_i==1, 0] = X_nb.squeeze()\n",
    "Y[m_i==1, 0] = Y_nb.squeeze()\n",
    "\n",
    "X[:, 1] = m_i\n",
    "\n",
    "# normalize \n",
    "X[:, 0], norm = normalize(X[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and train a general MDN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "model = MDN(ndim_input=2, n_components=n_components)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train(X, Y, n_epochs=100, n_minibatch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate test data with target parameters \n",
    "true_lam = 4.\n",
    "X_o_poisson = scipy.stats.poisson.rvs(mu=true_lam, size=sample_size)\n",
    "X_var_poisson = Variable(torch.Tensor(X_o_poisson.astype(float)))\n",
    "\n",
    "true_p = 0.7\n",
    "X_o_nb = scipy.stats.nbinom.rvs(n=3., p=true_p, size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now evaluate the model at the observed data \n",
    "stats_o_poisson = calculate_stats(X_o_poisson).astype(float).reshape(1, 1)\n",
    "stats_o_poisson, norm = normalize(stats_o_poisson, norm)\n",
    "# add the model identifier \n",
    "stats_o_poisson = np.array([[stats_o_poisson[0][0], -1.]])\n",
    "\n",
    "stats_o_nb = calculate_stats(X_o_nb).astype(float).reshape(1, 1)\n",
    "stats_o_nb, norm = normalize(stats_o_nb, norm)\n",
    "# add the model identifier \n",
    "stats_o_nb = np.array([[stats_o_nb[0][0], 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_MoG_posterior(model, stats_o, thetas):  \n",
    "    \n",
    "    (out_alpha, out_sigma, out_mu) = model(Variable(torch.Tensor(stats_o))) \n",
    "    \n",
    "    # get parameters in torch format\n",
    "    torch_thetas = Variable(torch.Tensor(thetas)).unsqueeze(1)\n",
    "    sigmas = out_sigma.expand(n_thetas, n_components)\n",
    "    mus = out_mu.expand(n_thetas, n_components)\n",
    "    alphas = out_alpha.expand(n_thetas, n_components)\n",
    "    \n",
    "    # get predicted posterior as MoG \n",
    "    post = mog_pdf_new(y=torch_thetas, sigmas=sigmas, mus=mus, alphas=alphas)\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_thetas = 1000\n",
    "thetas_poisson = np.linspace(0, 15, n_thetas)\n",
    "# the parameters wrt to summary stats is eta = N * lambda\n",
    "etas_poisson = sample_size * thetas_poisson\n",
    "\n",
    "thetas_nb = np.linspace(0, 1, n_thetas)\n",
    "post_poisson = get_MoG_posterior(model, stats_o_poisson, thetas_poisson)\n",
    "post_nb = get_MoG_posterior(model, stats_o_nb, thetas_nb)\n",
    "\n",
    "# get true posteriors \n",
    "# get analytical gamma posterior \n",
    "k_post = shape + np.sum(X_o_poisson)\n",
    "\n",
    "# use the posterior given the summary stats, not the data vector \n",
    "#scale_post = 1. / (sample_size + scale**-1)\n",
    "scale_post = 1. / (1 + (scale * sample_size)**-1)\n",
    "\n",
    "# somehow we have to scale with N again, why? because the scale is changed due to s(x) by 1/N  \n",
    "true_post_poisson = sample_size * gamma.pdf(x=etas_poisson, a=k_post, scale=scale_post)\n",
    "\n",
    "# get analytical beta posterior \n",
    "a = alp + sample_size * r\n",
    "b = bet + X_o_nb.sum()\n",
    "true_post_nb = beta.pdf(thetas_nb, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(r'Gamma posterior fit for Poisson $\\lambda$')\n",
    "plt.plot(thetas_poisson, gamma_prior.pdf(thetas_poisson), label='prior')\n",
    "plt.plot(thetas_poisson, true_post_poisson, label='analytical posterior')\n",
    "plt.plot(thetas_poisson, post_poisson.data.numpy(), label='predicted posterior')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\lambda$');\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(r'Beta posterior fit for negative binomial $p$')\n",
    "thetas = np.linspace(0, 1, n_thetas)\n",
    "plt.plot(thetas_nb, beta_prior.pdf(thetas), label='prior')\n",
    "plt.plot(thetas_nb, true_post_nb, label='analytical posterior')\n",
    "plt.plot(thetas_nb, post_nb.data.numpy(), label='predicted posterior')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$p$')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/universal_mdn_results.png', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
