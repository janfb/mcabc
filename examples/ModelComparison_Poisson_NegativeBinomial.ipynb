{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mcabc.model.PoissonModel import PoissonModel\n",
    "from mcabc.model.NegativeBinomialModel import NegativeBinomialModel\n",
    "from mcabc.mdn.Trainer import Trainer\n",
    "from mcabc.mdn.MixtureDensityNetwork import ClassificationMDN, UnivariateMogMDN, MultivariateMogMDN\n",
    "\n",
    "from mcabc.utils.stats import calculate_nb_evidence, poisson_evidence, calculate_nb_evidence, calculate_pprob_from_evidences\n",
    "from mcabc.utils.processing import generate_poisson_nb_data_set, calculate_stats_toy_examples, normalize\n",
    "\n",
    "\n",
    "#from delfi.distribution.mixture import MoG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Poisson and NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of samples per data point\n",
    "sample_size = 10\n",
    "# number of training data points\n",
    "ntrain = 100000\n",
    "# number of testing data points\n",
    "ntest = 100\n",
    "\n",
    "# prior hyper parameters : k2 small --> difficult, k2 large --> easy\n",
    "# NB model Gamma shape\n",
    "k2 = 20\n",
    "theta2 = 1.0\n",
    "# NB model Gamma scale\n",
    "k3 = 2.\n",
    "theta3 = 2.\n",
    "# Poisson model Gamma prior scale\n",
    "theta1 = 2.0\n",
    "# set Gamma prior shape such that the expected sample means of both model are equal:\n",
    "k1 = (k2 * theta2 * k3 * theta3) / theta1\n",
    "\n",
    "# define the models\n",
    "model_poisson = PoissonModel(sample_size=sample_size)\n",
    "model_nb = NegativeBinomialModel(sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate parameters from the priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the prior with the above hyper parameters\n",
    "prior_lam = scipy.stats.gamma(a=k1, scale=theta1) # gamma prior for Poisson model\n",
    "prior_k = scipy.stats.gamma(a=k2, scale=theta2) # gamma prior on Gamma distribution shape for the NB model\n",
    "prior_theta = scipy.stats.gamma(a=k3, scale=theta3) # gamma prior on Gamma distribution scale for the NB model\n",
    "\n",
    "# draw params from priors\n",
    "n = ntrain + ntest\n",
    "params_poi = prior_lam.rvs(size=int(n / 2))\n",
    "params_nb = np.vstack((prior_k.rvs(size=int(n / 2)),\n",
    "                       prior_theta.rvs(size=int(n / 2)))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data from models and calculate summary stats, prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate all data\n",
    "data_poi = model_poisson.gen(params_poi)\n",
    "data_nb = model_nb.gen(params_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle and set up model index target vector\n",
    "x_all = np.vstack((data_poi, data_nb))\n",
    "\n",
    "# define model indices\n",
    "m_all = np.hstack((np.zeros(data_poi.shape[0]), np.ones(data_nb.shape[0]))).squeeze().astype(int)\n",
    "\n",
    "# get shuffled indices\n",
    "shuffle_indices = np.arange(n)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "# shuffle the data\n",
    "x_all = x_all[shuffle_indices, ]\n",
    "m_all = m_all[shuffle_indices].tolist()\n",
    "\n",
    "# separate into training and testing data\n",
    "x, xtest = x_all[:ntrain, :], x_all[ntrain:, :]\n",
    "m, mtest = m_all[:ntrain], m_all[ntrain:]\n",
    "\n",
    "# calculate summary stats\n",
    "sx = calculate_stats_toy_examples(x)\n",
    "sx_test = calculate_stats_toy_examples(xtest)\n",
    "\n",
    "# use training norm to normalize test data\n",
    "sx_zt, training_norm = normalize(sx)\n",
    "sx_test_zt, training_norm = normalize(sx_test, training_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the MDN and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ClassificationMDN(n_input=2, n_hidden_units=10, n_hidden_layers=1)\n",
    "trainer = Trainer(model,\n",
    "                  optimizer=torch.optim.Adam(model.parameters(), lr=0.01), verbose=True, classification=True)\n",
    "\n",
    "n_epochs = 100\n",
    "n_minibatch = int(ntrain / 100)\n",
    "\n",
    "# train with training data\n",
    "loss_trace = trainer.train(sx_zt, m, n_epochs=n_epochs, n_minibatch=n_minibatch)\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the exact posterior probabilities for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppoi_exact = []\n",
    "# progress bar\n",
    "with tqdm(total=len(mtest), desc='Calculate evidences for test samples', ncols=110) as pbar:\n",
    "    # for every test sample\n",
    "    for xi in xtest:\n",
    "        # calcuate the evidences\n",
    "        nb_logevi = calculate_nb_evidence(xi, k2, theta2, k3, theta3, log=True)\n",
    "        poi_logevi = poisson_evidence(xi, k=k1, theta=theta1, log=True)\n",
    "        # calculate the posterior prob from the model evidences, given the prior is uniform.\n",
    "        ppoi_exact.append(calculate_pprob_from_evidences(np.exp(poi_logevi), np.exp(nb_logevi)))\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the MDN input output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a grid of means and variances as input to the MDN\n",
    "ms, vs = np.meshgrid(np.linspace(0, 300, 100), np.linspace(0, 300, 100))\n",
    "# stack values to evaluate as vector in the model\n",
    "sx_vis = np.vstack((ms.flatten(), vs.flatten())).T\n",
    "# normalize\n",
    "sx_vis, training_norm = normalize(sx_vis, training_norm)\n",
    "\n",
    "# predict probs\n",
    "ppoi_vec = model.predict(sx_vis)\n",
    "# take poisson posterior prob and reshape to grid dimensions\n",
    "ppoi_vismat = ppoi_vec[:, 0].reshape(ms.shape[0], vs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "# define color norm\n",
    "cnorm = mpl.colors.Normalize(vmin=ppoi_vismat.min(), vmax=ppoi_vismat.max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# plot test data points with true posterior probability as color code\n",
    "plt.scatter(x=sx_test[:, 0], y=sx_test[:, 1], c=np.array(ppoi_exact), cmap=cmap,\n",
    "            norm=cnorm, edgecolors='r', linewidths=0.3)\n",
    "# plot predicted posterior probs for the entire grid with same color code\n",
    "plt.imshow(ppoi_vismat, origin='lower', aspect='auto',\n",
    "           norm=cnorm, cmap=cmap, extent=[ms.min(), ms.max(), vs.min(), vs.max()])\n",
    "plt.xlabel('Sample mean', fontsize=15)\n",
    "plt.ylabel('Sample variance', fontsize=15)\n",
    "plt.colorbar(label='P(Poisson | x)', pad=0.01)\n",
    "plt.legend(['Exact posterior probabilities'], fontsize=15)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe some Poisson data and predict the posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a single sample of Poisson data, with lambda at the mean of the model prior\n",
    "true_lambda = scipy.stats.gamma.rvs(a=k1, scale=theta1)\n",
    "x_obs = scipy.stats.poisson.rvs(mu=true_lambda, size=sample_size)\n",
    "# calculate stats\n",
    "sx_obs = calculate_stats_toy_examples(x_obs)\n",
    "# normalize using training data normalization\n",
    "sx_obs_zt, training_norm = normalize(sx_obs, training_norm)\n",
    "\n",
    "# predict\n",
    "p_vec = model.predict(sx_obs_zt).squeeze()\n",
    "# calculate exact evidence\n",
    "nb_evidence = calculate_nb_evidence(x_obs, k2, theta2, k3, theta3, log=True)\n",
    "poi_evidence = poisson_evidence(x_obs, k=k1, theta=theta1, log=True)\n",
    "ppoi_ana = calculate_pprob_from_evidences(np.exp(poi_evidence), np.exp(nb_evidence))\n",
    "\n",
    "print(r'predicted P(poisson | data) = {:.2f}'.format(p_vec[0]))\n",
    "print(r'exact P(poisson | data) = {:.2f}'.format(ppoi_ana))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the high posterior probability for the Poisson model we predict the posterior over its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG\n",
    "model_params_mdn = UnivariateMogMDN(ndim_input=2, n_hidden=10, n_components=2)\n",
    "optimizer = torch.optim.Adam(model_params_mdn.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_params_mdn, optimizer, verbose=True)\n",
    "\n",
    "# calculate stats for poisson model\n",
    "sx_poi = calculate_stats_toy_examples(data_poi)\n",
    "# normalize data\n",
    "sx_poi_zt, data_norm = normalize(sx_poi)\n",
    "# normalize prior params\n",
    "params_poi_zt, prior_norm = normalize(params_poi)\n",
    "\n",
    "loss_trace = trainer.train(sx_poi_zt, params_poi_zt, n_epochs=100, n_minibatch=int(n_minibatch))\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the observed data\n",
    "sx_obs_zt, data_norm = normalize(sx_obs, data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict posterior for observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicte the posterior for the observed data, this is in normalized parameter space\n",
    "predicted_posterior = model_params_mdn.predict(sx_obs_zt)\n",
    "# transform back to actual parameter space\n",
    "predicted_posterior = predicted_posterior.ztrans_inv(mean=prior_norm[0], std=prior_norm[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate analytical posterior for observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get analytical gamma posterior\n",
    "k_post = k1 + np.sum(x_obs)\n",
    "\n",
    "# use the posterior given the summary stats, not the data vector\n",
    "scale_post = 1. / (sample_size + theta1**-1)\n",
    "exact_posterior = scipy.stats.gamma(a=k_post, scale=scale_post)\n",
    "\n",
    "# create a range of parameters for plotting the two posteriors\n",
    "thetas = np.linspace(true_lambda - 10, true_lambda + 10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.title(r'Gamma posterior fit for Poisson $\\lambda$')\n",
    "\n",
    "# plt.plot(thetas_poisson, gamma_prior.pdf(thetas_poisson), label='prior')\n",
    "plt.plot(thetas, exact_posterior.pdf(thetas), label='analytical posterior')\n",
    "plt.plot(thetas, predicted_posterior.eval_numpy(thetas), label='predicted posterior')\n",
    "plt.axvline(x=k1 * theta1, color=\"C2\", label='true lambda')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel(r'$\\lambda$', fontsize=15)\n",
    "plt.ylabel('Posterior density', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also observe NB data and learn the posterior over the parameters of the NB model:\n",
    "The NB posterior is 2D, one dimension for $k$ and once for $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a network to approximate the posterior with a MoG\n",
    "model_params_mdn = MultivariateMogMDN(ndim_input=2, ndim_output=2, n_hidden_units=10,\n",
    "                                      n_hidden_layers=2, n_components=2)\n",
    "optimizer = torch.optim.Adam(model_params_mdn.parameters(), lr=0.01)\n",
    "trainer = Trainer(model_params_mdn, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate stats for poisson model\n",
    "sx_nb = calculate_stats_toy_examples(data_nb)\n",
    "# normalize data\n",
    "sx_nb_zt, data_norm = normalize(sx_nb)\n",
    "# normalize prior params\n",
    "params_nb_zt, prior_norm = normalize(params_nb)\n",
    "\n",
    "loss_trace = trainer.train(sx_nb_zt, params_nb_zt, n_epochs=200, n_minibatch=200)\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(loss_trace)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# observe new NB data, normalize with norms from training data\n",
    "thetao = [[prior_k.rvs(), prior_theta.rvs()]]\n",
    "xo = model_nb.gen(thetao)\n",
    "sxo = calculate_stats_toy_examples(xo)\n",
    "sxo_zt, _ = normalize(sxo, data_norm)\n",
    "thetao_zt, _ = normalize(thetao, prior_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict posterior and transform to absolut range\n",
    "predicted_posterior_nb = model_params_mdn.predict(sxo_zt).ztrans_inv(mean=prior_norm[0], std=prior_norm[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate exact NB posterior\n",
    "Because the NB distribution as defined here is not in the exponential family, there is no closed form posterior. We approximate the exact solution numerically and visualize the resulting posterior as a 2D histogram."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
